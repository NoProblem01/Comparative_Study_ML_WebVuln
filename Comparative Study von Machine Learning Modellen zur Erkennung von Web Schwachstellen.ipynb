{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41cdae0",
   "metadata": {},
   "source": [
    "# Code zu der Bachelorarbeit:\n",
    "# \"Comparitve Study von Machine Learning Modellen zur Erkennung von Web Schwachstellen\"\n",
    "## von Nils Pudenz, 2735230"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21512b38",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dac7f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%pip install kaggle scikit-learn xgboost catboost tabpfn pandas numpy matplotlib seaborn -q\n",
    "#%pip install --quiet scikit-learn xgboost catboost tabpfn chardet\n",
    "#%pip install -U scikit-learn\n",
    "## in deiner (Conda/venv) Umgebung\n",
    "#%pip install --upgrade \"torch==2.*\" --index-url https://download.pytorch.org/whl/cu121\n",
    "#%pip install --upgrade xgboost catboost scikit-learn pandas scipy tabpfn\n",
    "##wenn es komplikationen mit torch gibt, deiinstallieren und neu installieren\n",
    "#%pip uninstall torch\n",
    "#%pip install torch --index-url https://download.pytorch.org/whl/cu121 --upgrade\n",
    "#%pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe89a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import zipfile\n",
    "import random\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tabpfn import TabPFNClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils import resample\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915add7c",
   "metadata": {},
   "source": [
    "## Check ob GPU verwendet werden kann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5103769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n",
      "CUDA verfügbar: True\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)\n",
    "print(\"CUDA verfügbar:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b9e350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: c:\\Users\\nilsp\\Github_Desktop\\Comparative_Study_ML_WebVuln\\.venv\\Scripts\\python.exe\n",
      "Torch: 2.5.1+cu121 | CUDA build: 12.1\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Python:\", sys.executable)\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA build:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4fea7e",
   "metadata": {},
   "source": [
    "## mögliche Modeloptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47f16504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nxgb = XGBClassifier(\\n    tree_method=\"hist\",        # CPU-Optimierung\\n    n_estimators=400,\\n    max_depth=6,\\n    learning_rate=0.1,\\n    subsample=0.9,\\n    colsample_bytree=0.8,\\n    n_jobs=-1,\\n    random_state=42\\n)\\n\\nfrom catboost import CatBoostClassifier\\ncat = CatBoostClassifier(\\n    iterations=400,\\n    depth=8,\\n    learning_rate=0.1,\\n    random_seed=42,\\n    loss_function=\"Logloss\",\\n    task_type=\"CPU\",\\n    thread_count=8,\\n    od_type=\"Iter\",            # early stopping\\n    od_wait=30,\\n    verbose=False\\n)\\n#'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "xgb = XGBClassifier(\n",
    "    tree_method=\"hist\",        # CPU-Optimierung\n",
    "    n_estimators=400,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.8,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "cat = CatBoostClassifier(\n",
    "    iterations=400,\n",
    "    depth=8,\n",
    "    learning_rate=0.1,\n",
    "    random_seed=42,\n",
    "    loss_function=\"Logloss\",\n",
    "    task_type=\"CPU\",\n",
    "    thread_count=8,\n",
    "    od_type=\"Iter\",            # early stopping\n",
    "    od_wait=30,\n",
    "    verbose=False\n",
    ")\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "407c50e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvec = TfidfVectorizer(\\n    ngram_range=(3,5),\\n    max_features=50_000,\\n    sublinear_tf=True,\\n    lowercase=False\\n).fit(X_train_raw)           # nur einmal fitten\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Speicher optimieren\n",
    "'''\n",
    "vec = TfidfVectorizer(\n",
    "    ngram_range=(3,5),\n",
    "    max_features=50_000,\n",
    "    sublinear_tf=True,\n",
    "    lowercase=False\n",
    ").fit(X_train_raw)           # nur einmal fitten\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fb31025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsearch_space = {\\n    \"max_depth\": [4, 6, 8],\\n    \"learning_rate\": [0.05, 0.1, 0.2],\\n    \"n_estimators\": [200, 400, 600]\\n}\\nrandcv = RandomizedSearchCV(\\n    xgb,\\n    search_space,\\n    n_iter=10,            # statt 3×3×3 = 27\\n    scoring=\"f1\",\\n    cv=3,\\n    n_jobs=-1\\n)\\nrandcv.fit(X_train_vec, y_train)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#kleine Hyperparamsuche statt Grid-Overkill\n",
    "'''\n",
    "search_space = {\n",
    "    \"max_depth\": [4, 6, 8],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "    \"n_estimators\": [200, 400, 600]\n",
    "}\n",
    "randcv = RandomizedSearchCV(\n",
    "    xgb,\n",
    "    search_space,\n",
    "    n_iter=10,            # statt 3×3×3 = 27\n",
    "    scoring=\"f1\",\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "randcv.fit(X_train_vec, y_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7ba5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d283a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_in_batches(model, X, batch_size=512): #um Überlastung zu vermeiden, evtl. 256 oder 128\n",
    "    \"\"\"Make predictions on input data in batches.\"\"\"\n",
    "    preds = []\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        batch = X[i:i + batch_size]\n",
    "        preds.append(model.predict(batch))\n",
    "    return np.concatenate(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0350a6b7",
   "metadata": {},
   "source": [
    "## Dowload Kaggle Datasets\n",
    "Requires Kaggle API credentials ('~/.kaggle/kaggle.json') für API-Token, um zugriff auf die Datenbanken über das Kaggle Konto zu bekommen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "749404d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e33f5a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dowload der Datasets von Kaggle, Output =1 erfoglgreich, Output = 0 fehlerhaft\n",
    "os.system(\"kaggle datasets download -d syedsaqlainhussain/sql-injection-dataset -p data --unzip --quiet\")\n",
    "os.system(\"kaggle datasets download -d syedsaqlainhussain/cross-site-scripting-xss-dataset-for-deep-learning -p data --unzip --quiet\")\n",
    "#KAGGLE_DATASETS = { #gleich wie oben nur renaming auf sql & xss\n",
    "#    \"sql\": \"syedsaqlainhussain/sql-injection-dataset\",\n",
    "#    \"xss\": \"syedsaqlainhussain/cross-site-scripting-xss-dataset-for-deep-learning\"\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fe4a3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/syedsaqlainhussain/sql-injection-dataset\n",
      "Downloaded syedsaqlainhussain/sql-injection-dataset\n",
      "Dataset URL: https://www.kaggle.com/datasets/syedsaqlainhussain/cross-site-scripting-xss-dataset-for-deep-learning\n",
      "Downloaded syedsaqlainhussain/cross-site-scripting-xss-dataset-for-deep-learning\n"
     ]
    }
   ],
   "source": [
    "def kaggle_download(dataset, path=\"data\", unzip=True):\n",
    "    api = KaggleApi()\n",
    "    api.authenticate() #nutzt ~/.kaggle/kaggle.json für Authentifizierung oder Environment-Variablen\n",
    "    api.dataset_download_files(dataset, path=path, unzip=unzip)\n",
    "    print(f\"Downloaded {dataset}\")\n",
    "\n",
    "kaggle_download(\"syedsaqlainhussain/sql-injection-dataset\")#, path=DATA_DIR, unzip=True)\n",
    "kaggle_download(\"syedsaqlainhussain/cross-site-scripting-xss-dataset-for-deep-learning\")#, path=DATA_DIR, unzip=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51b9480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sql_df = pd.read_csv(\"data/SQLiV3.csv\", encoding=\"utf-8\", low_memory=False)\n",
    "xss_df = pd.read_csv(\"data/XSS_dataset.csv\", encoding=\"utf-8\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d3a3440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentence', 'Label', 'Unnamed: 2', 'Unnamed: 3']\n",
      "clean shape: (30873, 2)\n",
      "Label\n",
      "0                                                                         0.628891\n",
      "1                                                                         0.370162\n",
      " --                                                                       0.000359\n",
      "waitfor delay '0:0:__TIME__'--                                            0.000131\n",
      " DROP TABLE Suppliers                                                     0.000065\n",
      " desc users                                                               0.000033\n",
      "SELECT *                                                                  0.000033\n",
      " OR                                                                       0.000033\n",
      " if not  (  select system_user  )   <> 'sa' waitfor delay '0:0:2' --      0.000033\n",
      " drop table temp --                                                       0.000033\n",
      " grant resource to name                                                   0.000033\n",
      " /*Select all the columns of all the records in the Customers table:*/    0.000033\n",
      " EXEC SelectAllCustomers                                                  0.000033\n",
      "*/                                                                        0.000033\n",
      " CREATE VIEW [Products Above Average Price] AS                            0.000033\n",
      " CREATE OR REPLACE VIEW view_name AS                                      0.000033\n",
      " SELECT column_name ( s )                                                 0.000033\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Spalten ansehen\n",
    "print(sql_df.columns.tolist())\n",
    "\n",
    "# Typische Index-/Hilfsspalten loswerden\n",
    "sql_df = sql_df.loc[:, ~sql_df.columns.str.contains(r\"^Unnamed|^index$\", case=False)]\n",
    "\n",
    "# Auf die Kernspalten reduzieren (falls etwas anderes drin ist)\n",
    "sql_df = sql_df[[\"Sentence\", \"Label\"]].copy()\n",
    "\n",
    "# Optional: Duplikate auf Satzebene entfernen (falls noch nicht passiert)\n",
    "sql_df = sql_df.drop_duplicates(subset=[\"Sentence\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"clean shape:\", sql_df.shape)  # Erwartung: (30873, 2)\n",
    "print(sql_df[\"Label\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36a79aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL dataset shape: (30873, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\" or pg_sleep  (  __TIME__  )  --</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>create user name identified by pass123 tempora...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AND 1  =  utl_inaddr.get_host_address   (    ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>select * from users where id  =  '1' or @ @1 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>select * from users where id  =  1 or 1#\"  ( ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Label\n",
       "0                  \" or pg_sleep  (  __TIME__  )  --     1\n",
       "1  create user name identified by pass123 tempora...   NaN\n",
       "2   AND 1  =  utl_inaddr.get_host_address   (    ...     1\n",
       "3   select * from users where id  =  '1' or @ @1 ...     1\n",
       "4   select * from users where id  =  1 or 1#\"  ( ...     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30872</td>\n",
       "      <td>30619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>30872</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>\" or pg_sleep  (  __TIME__  )  --</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>19256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Sentence  Label\n",
       "count                               30872  30619\n",
       "unique                              30872     17\n",
       "top     \" or pg_sleep  (  __TIME__  )  --      0\n",
       "freq                                    1  19256"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30873 entries, 0 to 30872\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sentence  30872 non-null  object\n",
      " 1   Label     30619 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 482.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XSS dataset shape: (13686, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;li&gt;&lt;a href=\"/wiki/File:Socrates.png\" class=\"i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;tt onmouseover=\"alert(1)\"&gt;test&lt;/tt&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\t &lt;/span&gt; &lt;span class=\"reference-text\"&gt;Steeri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\t &lt;/span&gt; &lt;span class=\"reference-text\"&gt;&lt;cite ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\\t &lt;/span&gt;. &lt;a href=\"/wiki/Digital_object_iden...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           Sentence  Label\n",
       "0           0  <li><a href=\"/wiki/File:Socrates.png\" class=\"i...      0\n",
       "1           1               <tt onmouseover=\"alert(1)\">test</tt>      1\n",
       "2           2  \\t </span> <span class=\"reference-text\">Steeri...      0\n",
       "3           3  \\t </span> <span class=\"reference-text\"><cite ...      0\n",
       "4           4  \\t </span>. <a href=\"/wiki/Digital_object_iden...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13686.000000</td>\n",
       "      <td>13686.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6842.500000</td>\n",
       "      <td>0.538726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3950.952227</td>\n",
       "      <td>0.498516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3421.250000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6842.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10263.750000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13685.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0         Label\n",
       "count  13686.000000  13686.000000\n",
       "mean    6842.500000      0.538726\n",
       "std     3950.952227      0.498516\n",
       "min        0.000000      0.000000\n",
       "25%     3421.250000      0.000000\n",
       "50%     6842.500000      1.000000\n",
       "75%    10263.750000      1.000000\n",
       "max    13685.000000      1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13686 entries, 0 to 13685\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  13686 non-null  int64 \n",
      " 1   Sentence    13686 non-null  object\n",
      " 2   Label       13686 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 320.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for name, df in {\"SQL\": sql_df, \"XSS\": xss_df}.items():\n",
    "    print(f\"{name} dataset shape: {df.shape}\")\n",
    "    display(df.head())\n",
    "    display(df.describe())\n",
    "    display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1d30aa",
   "metadata": {},
   "source": [
    "## Basic Cleaning\n",
    "* Drop Duplicate rows\n",
    "* Handle missing values (simple fill-na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce598302",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (sql_df, xss_df):\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5eedf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_xy(df: pd.DataFrame,\n",
    "                  label_candidates=(\"label\", \"class\", \"target\"),\n",
    "                  label_map=None):\n",
    "    if label_map is None:\n",
    "        label_map = {\n",
    "            \"0\": \"0\", \"1\": \"1\",\n",
    "            \"benign\": \"0\", \"normal\": \"0\", \"legitimate\": \"0\", \"safe\": \"0\",\n",
    "            \"attack\": \"1\", \"malicious\": \"1\", \"sql injection\": \"1\",\n",
    "            \"sql-injection\": \"1\", \"xss\": \"1\"\n",
    "        }\n",
    "\n",
    "    # Zielspalte finden (im *übergebenen* df!)\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    target_col = next((cols_lower[c] for c in label_candidates if c in cols_lower), None)\n",
    "    if target_col is None:\n",
    "        raise ValueError(f\"Keine Label-Spalte gefunden. Kandidaten: {label_candidates}\")\n",
    "\n",
    "    # Labels normieren -> nur 0/1 behalten\n",
    "    y_str = df[target_col].astype(str).str.strip().str.lower()\n",
    "    y_map = y_str.map(label_map)\n",
    "    mask = y_map.notna()\n",
    "    y = pd.to_numeric(y_map[mask]).astype(int).to_numpy()\n",
    "\n",
    "    # Rohtext aus allen Nicht-Label-Spalten zusammenbauen\n",
    "    feat_cols = [c for c in df.columns if c != target_col]\n",
    "    X_raw = df.loc[mask, feat_cols].astype(str).agg(\" \".join, axis=1)\n",
    "\n",
    "    return X_raw, y, target_col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ce09f",
   "metadata": {},
   "source": [
    "# Globale Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6a3f45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ENV] CUDA avail: True | Torch CUDA build: 12.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Deterministische Ausgabe generieren, um die Reproduzierbarkeit zu gewährleisten\n",
    "RANDOM_STATE = 42\n",
    "#np.random.seed(RANDOM_STATE) wofür das?\n",
    "#random.seed(RANDOM_STATE)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"8\"\n",
    "torch.set_num_threads(8)\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()  # für TabPFN & XGBoost\n",
    "print(f\"[ENV] CUDA avail: {USE_CUDA} | Torch CUDA build: {torch.version.cuda}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee6d9eb",
   "metadata": {},
   "source": [
    "# Hilfsfunktionen "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790bf05a",
   "metadata": {},
   "source": [
    "## Evaluationsmetriken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba189305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_metrics(y_true, y_pred) -> dict:\n",
    "    \"\"\"Präzision/Recall/F1 & Raten (FPR/FNR) für binäre Klassifikation.\"\"\"\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    p  = precision_score(y_true, y_pred)\n",
    "    r  = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    fpr = float(fp / (fp + tn)) if (fp + tn) else 0.0\n",
    "    fnr = float(fn / (fn + tp)) if (fn + tp) else 0.0\n",
    "    return dict(Precision=p, Recall=r, F1=f1, FPR=fpr, FNR=fnr)\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, name, use_batches=False, batch_size=256) -> dict: #Dictionary für Evaluierungsmetriken\n",
    "    \"\"\"Zeitmessung + Vorhersage (optional in Batches) + Metriken.\"\"\"\n",
    "    print(f\"→ Evaluate {name} on X_test={getattr(X_test,'shape',None)}\")\n",
    "    t0 = time.perf_counter()\n",
    "    if use_batches:\n",
    "        y_pred = predict_in_batches(model, X_test, batch_size=batch_size, verbose=True)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "    pred_s = time.perf_counter() - t0\n",
    "    m = binary_metrics(y_test, y_pred)\n",
    "    res = dict(Model=name, Pred_s=pred_s, **m)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7c572f",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b10bd60",
   "metadata": {},
   "source": [
    "### Batchweise Vorhersage für TabPFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9fc7885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_in_batches(model, X, batch_size=256, verbose=False):\n",
    "    \"\"\"Vorhersage in Batches (schont RAM/VRAM; wichtig für TabPFN).\"\"\"\n",
    "    n = X.shape[0]\n",
    "    out = []\n",
    "    total = math.ceil(n / batch_size)\n",
    "    for b, i in enumerate(range(0, n, batch_size), start=1):\n",
    "        j = min(i + batch_size, n)\n",
    "        t1 = time.perf_counter()\n",
    "        with torch.inference_mode():\n",
    "            out.append(model.predict(X[i:j]))\n",
    "        dt = time.perf_counter() - t1\n",
    "        if verbose:\n",
    "            print(f\"   [predict] batch {b:>3}/{total} ({j-i} rows) in {dt:.2f}s\")\n",
    "        if USE_CUDA:\n",
    "            torch.cuda.synchronize()\n",
    "    return np.concatenate(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2e87bd",
   "metadata": {},
   "source": [
    "### Label bereinigung der Datensets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bbb5358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_labels(df, label_col=\"Label\", text_cols=(\"Sentence\",)):\n",
    "    \"\"\"Bringt Labels robust auf {0,1} und gibt (X_raw, y) zurück.\"\"\"\n",
    "    df = df.copy()\n",
    "    y_raw = df[label_col].astype(str).str.strip().str.lower()\n",
    "    map01 = {\"0\": \"0\", \"1\": \"1\", \"benign\": \"0\", \"normal\": \"0\", \"legitimate\": \"0\", \"safe\": \"0\",\n",
    "             \"attack\": \"1\", \"malicious\": \"1\", \"sql injection\": \"1\", \"sql-injection\": \"1\", \"xss\": \"1\"}\n",
    "    y_map = y_raw.map(map01)\n",
    "    mask = y_map.notna()\n",
    "    y = pd.to_numeric(y_map[mask]).astype(int).to_numpy()\n",
    "    feat_cols = [c for c in df.columns if c != label_col]\n",
    "    X_raw = df.loc[mask, feat_cols].astype(str).agg(\" \".join, axis=1)\n",
    "    return X_raw, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe7b6ec",
   "metadata": {},
   "source": [
    "# Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18928fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=RANDOM_STATE),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(256,128), activation=\"relu\",\n",
    "                         early_stopping=True, n_iter_no_change=5, max_iter=200,\n",
    "                         random_state=RANDOM_STATE),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=500, max_depth=6, learning_rate=0.1,\n",
    "        subsample=0.9, colsample_bytree=0.8,\n",
    "        tree_method=\"hist\", device=(\"cuda\" if USE_CUDA else \"cpu\"),\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "        iterations=400, depth=8, learning_rate=0.1,\n",
    "        loss_function=\"Logloss\", random_seed=RANDOM_STATE, verbose=False,\n",
    "        task_type=\"CPU\"   # stabil über Pool + Sparse; bei GPU: task_type=\"GPU\" und ggf. Dense verwenden\n",
    "    ),\n",
    "    \"TabPFN\": TabPFNClassifier(\n",
    "        device=(\"cuda\" if USE_CUDA else \"cpu\"),\n",
    "        ignore_pretraining_limits=True\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690192f4",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ee0af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VEC_ARGS = dict(ngram_range=(3,5), max_features=50_000, sublinear_tf=True, lowercase=False)\n",
    "\n",
    "TABPFN_MAX_SAMPLES = 4000     # starte konservativ; später 6000/8000 testen\n",
    "TABPFN_N_COMPONENTS = 150     # <=500, kleiner = schneller/ram-sparender\n",
    "TABPFN_BATCH = 128            # Batch für Predict; 128/64 bei RAM-Engpässen\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3c27db",
   "metadata": {},
   "source": [
    "## Pipeline je Datensatz (TF-IDF; TabPFN: SVD + batched predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbcfd376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATASET: SQL\n",
      "[DATA] rows=30844 | pos=11334 | neg=19510\n",
      "[VEC] TF-IDF train=(24675, 50000), test=(6169, 50000) in 1.01s\n",
      "--------------------------------------------------\n",
      "MODEL: RandomForest on SQL\n",
      "[RandomForest] fit in 13.52s\n",
      "→ Evaluate RandomForest-SQL on X_test=(6169, 50000)\n",
      "{'Model': 'RandomForest-SQL', 'Pred_s': 0.8905150000937283, 'Precision': 0.9917172832689122, 'Recall': 0.792236435818262, 'F1': 0.8808239333006376, 'FPR': 0.0038441824705279346, 'FNR': 0.20776356418173797}\n",
      "--------------------------------------------------\n",
      "MODEL: MLP on SQL\n",
      "[MLP] fit in 796.96s\n",
      "→ Evaluate MLP-SQL on X_test=(6169, 50000)\n",
      "{'Model': 'MLP-SQL', 'Pred_s': 0.020672500133514404, 'Precision': 0.9911894273127754, 'Recall': 0.7940008822232024, 'F1': 0.881704628949302, 'FPR': 0.004100461301896463, 'FNR': 0.20599911777679752}\n",
      "--------------------------------------------------\n",
      "MODEL: XGBoost on SQL\n",
      "[XGBoost] fit in 9.93s\n",
      "→ Evaluate XGBoost-SQL on X_test=(6169, 50000)\n",
      "{'Model': 'XGBoost-SQL', 'Pred_s': 0.027806099969893694, 'Precision': 1.0, 'Recall': 0.4239082487869431, 'F1': 0.5954151177199505, 'FPR': 0.0, 'FNR': 0.5760917512130569}\n",
      "--------------------------------------------------\n",
      "MODEL: CatBoost on SQL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nilsp\\Github_Desktop\\Comparative_Study_ML_WebVuln\\.venv\\Lib\\site-packages\\xgboost\\core.py:729: UserWarning: [09:07:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CatBoost] fit in 133.82s\n",
      "→ Evaluate CatBoost-SQL on X_test=(6169, 50000)\n",
      "{'Model': 'CatBoost-SQL', 'Pred_s': 0.127218599896878, 'Precision': 1.0, 'Recall': 0.7878253198059109, 'F1': 0.8813224771773994, 'FPR': 0.0, 'FNR': 0.2121746801940891}\n",
      "--------------------------------------------------\n",
      "MODEL: TabPFN on SQL\n",
      "[TabPFN] train subset: 4000 rows\n",
      "[TabPFN] SVD train=(4000, 150), test=(6169, 150) in 2.64s\n",
      "[TabPFN] fit in 68.66s (device=cuda)\n",
      "→ Evaluate TabPFN-SQL on X_test=(6169, 150)\n",
      "   [predict] batch   1/49 (128 rows) in 31.92s\n",
      "   [predict] batch   2/49 (128 rows) in 36.72s\n",
      "   [predict] batch   3/49 (128 rows) in 30.10s\n",
      "   [predict] batch   4/49 (128 rows) in 29.47s\n",
      "   [predict] batch   5/49 (128 rows) in 29.71s\n",
      "   [predict] batch   6/49 (128 rows) in 29.67s\n",
      "   [predict] batch   7/49 (128 rows) in 29.93s\n",
      "   [predict] batch   8/49 (128 rows) in 29.66s\n",
      "   [predict] batch   9/49 (128 rows) in 29.78s\n",
      "   [predict] batch  10/49 (128 rows) in 29.84s\n",
      "   [predict] batch  11/49 (128 rows) in 29.66s\n",
      "   [predict] batch  12/49 (128 rows) in 29.91s\n",
      "   [predict] batch  13/49 (128 rows) in 29.80s\n",
      "   [predict] batch  14/49 (128 rows) in 29.90s\n",
      "   [predict] batch  15/49 (128 rows) in 29.76s\n",
      "   [predict] batch  16/49 (128 rows) in 29.82s\n",
      "   [predict] batch  17/49 (128 rows) in 29.66s\n",
      "   [predict] batch  18/49 (128 rows) in 29.96s\n",
      "   [predict] batch  19/49 (128 rows) in 29.73s\n",
      "   [predict] batch  20/49 (128 rows) in 29.67s\n",
      "   [predict] batch  21/49 (128 rows) in 29.73s\n",
      "   [predict] batch  22/49 (128 rows) in 29.77s\n",
      "   [predict] batch  23/49 (128 rows) in 42.51s\n",
      "   [predict] batch  24/49 (128 rows) in 32.10s\n",
      "   [predict] batch  25/49 (128 rows) in 33.03s\n",
      "   [predict] batch  26/49 (128 rows) in 27.74s\n",
      "   [predict] batch  27/49 (128 rows) in 27.85s\n",
      "   [predict] batch  28/49 (128 rows) in 27.75s\n",
      "   [predict] batch  29/49 (128 rows) in 28.00s\n",
      "   [predict] batch  30/49 (128 rows) in 27.63s\n",
      "   [predict] batch  31/49 (128 rows) in 28.01s\n",
      "   [predict] batch  32/49 (128 rows) in 27.73s\n",
      "   [predict] batch  33/49 (128 rows) in 28.30s\n",
      "   [predict] batch  34/49 (128 rows) in 28.24s\n",
      "   [predict] batch  35/49 (128 rows) in 27.90s\n",
      "   [predict] batch  36/49 (128 rows) in 28.28s\n",
      "   [predict] batch  37/49 (128 rows) in 27.97s\n",
      "   [predict] batch  38/49 (128 rows) in 28.30s\n",
      "   [predict] batch  39/49 (128 rows) in 27.95s\n",
      "   [predict] batch  40/49 (128 rows) in 27.87s\n",
      "   [predict] batch  41/49 (128 rows) in 27.82s\n",
      "   [predict] batch  42/49 (128 rows) in 27.79s\n",
      "   [predict] batch  43/49 (128 rows) in 28.07s\n",
      "   [predict] batch  44/49 (128 rows) in 27.90s\n",
      "   [predict] batch  45/49 (128 rows) in 27.85s\n",
      "   [predict] batch  46/49 (128 rows) in 27.89s\n",
      "   [predict] batch  47/49 (128 rows) in 27.95s\n",
      "   [predict] batch  48/49 (128 rows) in 27.87s\n",
      "   [predict] batch  49/49 (25 rows) in 114.31s\n",
      "{'Model': 'TabPFN-SQL', 'Pred_s': 1528.7960629998706, 'Precision': 1.0, 'Recall': 0.7838553153947949, 'F1': 0.8788328387734916, 'FPR': 0.0, 'FNR': 0.2161446846052051}\n",
      "\n",
      "======================================================================\n",
      "DATASET: XSS\n",
      "[DATA] rows=13686 | pos=7373 | neg=6313\n",
      "[VEC] TF-IDF train=(10948, 50000), test=(2738, 50000) in 1.02s\n",
      "--------------------------------------------------\n",
      "MODEL: RandomForest on XSS\n",
      "[RandomForest] fit in 1.49s\n",
      "→ Evaluate RandomForest-XSS on X_test=(2738, 50000)\n",
      "{'Model': 'RandomForest-XSS', 'Pred_s': 0.08669919986277819, 'Precision': 1.0, 'Recall': 0.9450847457627118, 'F1': 0.9717671662600209, 'FPR': 0.0, 'FNR': 0.05491525423728814}\n",
      "--------------------------------------------------\n",
      "MODEL: MLP on XSS\n",
      "[MLP] fit in 295.38s\n",
      "→ Evaluate MLP-XSS on X_test=(2738, 50000)\n",
      "{'Model': 'MLP-XSS', 'Pred_s': 0.009859399870038033, 'Precision': 0.999288256227758, 'Recall': 0.951864406779661, 'F1': 0.975, 'FPR': 0.000791765637371338, 'FNR': 0.04813559322033898}\n",
      "--------------------------------------------------\n",
      "MODEL: XGBoost on XSS\n",
      "[XGBoost] fit in 27.90s\n",
      "→ Evaluate XGBoost-XSS on X_test=(2738, 50000)\n",
      "{'Model': 'XGBoost-XSS', 'Pred_s': 0.020780500024557114, 'Precision': 0.6463628396143734, 'Recall': 1.0, 'F1': 0.7852009582113388, 'FPR': 0.6389548693586699, 'FNR': 0.0}\n",
      "--------------------------------------------------\n",
      "MODEL: CatBoost on XSS\n",
      "[CatBoost] fit in 126.12s\n",
      "→ Evaluate CatBoost-XSS on X_test=(2738, 50000)\n",
      "{'Model': 'CatBoost-XSS', 'Pred_s': 0.07833700021728873, 'Precision': 1.0, 'Recall': 0.9464406779661017, 'F1': 0.9724834552420759, 'FPR': 0.0, 'FNR': 0.05355932203389831}\n",
      "--------------------------------------------------\n",
      "MODEL: TabPFN on XSS\n",
      "[TabPFN] train subset: 4000 rows\n",
      "[TabPFN] SVD train=(4000, 150), test=(2738, 150) in 2.48s\n",
      "[TabPFN] fit in 9.89s (device=cuda)\n",
      "→ Evaluate TabPFN-XSS on X_test=(2738, 150)\n",
      "   [predict] batch   1/22 (128 rows) in 27.42s\n",
      "   [predict] batch   2/22 (128 rows) in 27.50s\n",
      "   [predict] batch   3/22 (128 rows) in 27.46s\n",
      "   [predict] batch   4/22 (128 rows) in 27.39s\n",
      "   [predict] batch   5/22 (128 rows) in 27.34s\n",
      "   [predict] batch   6/22 (128 rows) in 27.66s\n",
      "   [predict] batch   7/22 (128 rows) in 27.55s\n",
      "   [predict] batch   8/22 (128 rows) in 27.29s\n",
      "   [predict] batch   9/22 (128 rows) in 27.57s\n",
      "   [predict] batch  10/22 (128 rows) in 27.38s\n",
      "   [predict] batch  11/22 (128 rows) in 27.44s\n",
      "   [predict] batch  12/22 (128 rows) in 27.51s\n",
      "   [predict] batch  13/22 (128 rows) in 27.47s\n",
      "   [predict] batch  14/22 (128 rows) in 27.99s\n",
      "   [predict] batch  15/22 (128 rows) in 31.57s\n",
      "   [predict] batch  16/22 (128 rows) in 37.39s\n",
      "   [predict] batch  17/22 (128 rows) in 27.35s\n",
      "   [predict] batch  18/22 (128 rows) in 27.64s\n",
      "   [predict] batch  19/22 (128 rows) in 27.65s\n",
      "   [predict] batch  20/22 (128 rows) in 27.52s\n",
      "   [predict] batch  21/22 (128 rows) in 27.81s\n",
      "   [predict] batch  22/22 (50 rows) in 78.32s\n",
      "{'Model': 'TabPFN-XSS', 'Pred_s': 670.2129179001786, 'Precision': 1.0, 'Recall': 0.9423728813559322, 'F1': 0.9703315881326352, 'FPR': 0.0, 'FNR': 0.0576271186440678}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for df, ds_name in [(sql_df, \"SQL\"), (xss_df, \"XSS\")]:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"DATASET: {ds_name}\")\n",
    "\n",
    "    # Labels/Text\n",
    "    X_raw, y = clean_labels(df, label_col=\"Label\", text_cols=(\"Sentence\",))\n",
    "    print(f\"[DATA] rows={len(y)} | pos={int(y.sum())} | neg={len(y)-int(y.sum())}\")\n",
    "\n",
    "    # Split \n",
    "    # Erst den Datensatz splitten, um Data Leakage vorzubeugen, Wujek et al. (2016)\n",
    "    X_train_txt, X_test_txt, y_train, y_test = train_test_split(\n",
    "        X_raw, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    \n",
    "    # #Gemeinsmaer TF-IDF-Vectorizer (fit nur auf Train, wegen Oversampling), verhinderung von Fold Leakage\n",
    "    vec = TfidfVectorizer(**VEC_ARGS)\n",
    "    t0 = time.perf_counter()\n",
    "    X_train_vec = vec.fit_transform(X_train_txt)\n",
    "    X_test_vec  = vec.transform(X_test_txt)\n",
    "    print(f\"[VEC] TF-IDF train={X_train_vec.shape}, test={X_test_vec.shape} in {time.perf_counter()-t0:.2f}s\")\n",
    "\n",
    "    # Modelle trainieren/evaluieren\n",
    "    for name, model in models.items():\n",
    "        print(\"-\"*50 + f\"\\nMODEL: {name} on {ds_name}\")\n",
    "\n",
    "        if name == \"CatBoost\":\n",
    "            # CatBoost stabil via Pool (sparse)\n",
    "            pool_train = Pool(X_train_vec, y_train)\n",
    "            t0 = time.perf_counter()\n",
    "            model.fit(pool_train)\n",
    "            print(f\"[{name}] fit in {time.perf_counter()-t0:.2f}s\")\n",
    "            res = evaluate_model(model, X_test_vec, y_test, f\"{name}-{ds_name}\")\n",
    "            results.append(res)\n",
    "            print(res)\n",
    "\n",
    "        elif name == \"TabPFN\":\n",
    "            # ggf. Trainingsmenge stratifiziert klein halten\n",
    "            X_tab, y_tab = X_train_vec, y_train\n",
    "            if X_train_vec.shape[0] > TABPFN_MAX_SAMPLES:\n",
    "                sss = StratifiedShuffleSplit(n_splits=1, train_size=TABPFN_MAX_SAMPLES, random_state=RANDOM_STATE)\n",
    "                idx, _ = next(sss.split(X_train_vec, y_train))\n",
    "                X_tab = X_train_vec[idx]\n",
    "                y_tab = np.asarray(y_train)[idx]\n",
    "            print(f\"[{name}] train subset: {X_tab.shape[0]} rows\")\n",
    "\n",
    "            # SVD -> float32\n",
    "            t0 = time.perf_counter()\n",
    "            svd = TruncatedSVD(n_components=TABPFN_N_COMPONENTS, random_state=RANDOM_STATE)\n",
    "            X_train_svd = svd.fit_transform(X_tab).astype(\"float32\", copy=False)\n",
    "            X_test_svd  = svd.transform(X_test_vec).astype(\"float32\", copy=False)\n",
    "            print(f\"[{name}] SVD train={X_train_svd.shape}, test={X_test_svd.shape} in {time.perf_counter()-t0:.2f}s\")\n",
    "\n",
    "            # Fit\n",
    "            t0 = time.perf_counter()\n",
    "            model.fit(X_train_svd, y_tab)\n",
    "            if USE_CUDA:\n",
    "                torch.cuda.synchronize()\n",
    "            print(f\"[{name}] fit in {time.perf_counter()-t0:.2f}s (device={'cuda' if USE_CUDA else 'cpu'})\")\n",
    "\n",
    "            # Evaluate (batched predict; kein zweites predict in evaluate_model)\n",
    "            res = evaluate_model(model, X_test_svd, y_test, f\"{name}-{ds_name}\",\n",
    "                                 use_batches=True, batch_size=TABPFN_BATCH)\n",
    "            results.append(res)\n",
    "            print(res)\n",
    "\n",
    "        else:\n",
    "            # RF, MLP, XGB: direkt auf Sparse\n",
    "            t0 = time.perf_counter()\n",
    "            model.fit(X_train_vec, y_train)\n",
    "            print(f\"[{name}] fit in {time.perf_counter()-t0:.2f}s\")\n",
    "            res = evaluate_model(model, X_test_vec, y_test, f\"{name}-{ds_name}\")\n",
    "            results.append(res)\n",
    "            print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6574d3c4",
   "metadata": {},
   "source": [
    "# Ergebnisse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd8e411",
   "metadata": {},
   "source": [
    "## Ergebnistabelle ausgeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "173bb883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Pred_s</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>FPR</th>\n",
       "      <th>FNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoost-SQL</td>\n",
       "      <td>0.127219</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.787825</td>\n",
       "      <td>0.881322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatBoost-XSS</td>\n",
       "      <td>0.078337</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.946441</td>\n",
       "      <td>0.972483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP-SQL</td>\n",
       "      <td>0.020673</td>\n",
       "      <td>0.991189</td>\n",
       "      <td>0.794001</td>\n",
       "      <td>0.881705</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.205999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP-XSS</td>\n",
       "      <td>0.009859</td>\n",
       "      <td>0.999288</td>\n",
       "      <td>0.951864</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.048136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForest-SQL</td>\n",
       "      <td>0.890515</td>\n",
       "      <td>0.991717</td>\n",
       "      <td>0.792236</td>\n",
       "      <td>0.880824</td>\n",
       "      <td>0.003844</td>\n",
       "      <td>0.207764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForest-XSS</td>\n",
       "      <td>0.086699</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.945085</td>\n",
       "      <td>0.971767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TabPFN-SQL</td>\n",
       "      <td>1528.796063</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.783855</td>\n",
       "      <td>0.878833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TabPFN-XSS</td>\n",
       "      <td>670.212918</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.942373</td>\n",
       "      <td>0.970332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBoost-SQL</td>\n",
       "      <td>0.027806</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.423908</td>\n",
       "      <td>0.595415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.576092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBoost-XSS</td>\n",
       "      <td>0.020781</td>\n",
       "      <td>0.646363</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.785201</td>\n",
       "      <td>0.638955</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model       Pred_s  Precision    Recall        F1       FPR  \\\n",
       "0      CatBoost-SQL     0.127219   1.000000  0.787825  0.881322  0.000000   \n",
       "1      CatBoost-XSS     0.078337   1.000000  0.946441  0.972483  0.000000   \n",
       "2           MLP-SQL     0.020673   0.991189  0.794001  0.881705  0.004100   \n",
       "3           MLP-XSS     0.009859   0.999288  0.951864  0.975000  0.000792   \n",
       "4  RandomForest-SQL     0.890515   0.991717  0.792236  0.880824  0.003844   \n",
       "5  RandomForest-XSS     0.086699   1.000000  0.945085  0.971767  0.000000   \n",
       "6        TabPFN-SQL  1528.796063   1.000000  0.783855  0.878833  0.000000   \n",
       "7        TabPFN-XSS   670.212918   1.000000  0.942373  0.970332  0.000000   \n",
       "8       XGBoost-SQL     0.027806   1.000000  0.423908  0.595415  0.000000   \n",
       "9       XGBoost-XSS     0.020781   0.646363  1.000000  0.785201  0.638955   \n",
       "\n",
       "        FNR  \n",
       "0  0.212175  \n",
       "1  0.053559  \n",
       "2  0.205999  \n",
       "3  0.048136  \n",
       "4  0.207764  \n",
       "5  0.054915  \n",
       "6  0.216145  \n",
       "7  0.057627  \n",
       "8  0.576092  \n",
       "9  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ergebnisse gespeichert: results_all.csv / results_all.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Ergebnis-Tabelle ausgeben\n",
    "df_results = pd.DataFrame(results)\n",
    "display(df_results.sort_values([\"Model\"]).reset_index(drop=True))\n",
    "df_results.to_csv(\"results_all.csv\", index=False)\n",
    "with pd.ExcelWriter(\"results_all.xlsx\") as w:\n",
    "    df_results.to_excel(w, sheet_name=\"All\", index=False)\n",
    "print(\" Ergebnisse gespeichert: results_all.csv / results_all.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f721a",
   "metadata": {},
   "source": [
    "## Ergebnisse speichern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4caea88",
   "metadata": {},
   "source": [
    "## Ergebnistabelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0360229e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataFrame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results_df = pd-\u001b[43mDataFrame\u001b[49m(results)\n\u001b[32m      2\u001b[39m results_df.sort_values([\u001b[33m\"\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m\"\u001b[39m], inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Gesamt Ergebnisse: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, results_df)\n",
      "\u001b[31mNameError\u001b[39m: name 'DataFrame' is not defined"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.sort_values([\"Model\"], inplace=True)\n",
    "print(\"\\n Gesamt Ergebnisse: \\n\", results_df)\n",
    "\n",
    "results_df.to_csv(\"results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4015df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "# Variante A – wenn df_results bereits eine 'Dataset'-Spalte hat\n",
    "if \"Dataset\" in df_results.columns:\n",
    "    df_sql = df_results[df_results[\"Dataset\"] == \"SQL\"].copy()\n",
    "    df_xss = df_results[df_results[\"Dataset\"] == \"XSS\"].copy()\n",
    "# Variante B – Spalte fehlt -> am Modell-Namen aufteilen\n",
    "else:\n",
    "    df_sql = df_results[df_results[\"Model\"].str.contains(\"-SQL\")].copy()\n",
    "    df_xss = df_results[df_results[\"Model\"].str.contains(\"-XSS\")].copy()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) zwei CSV-Dateien \n",
    "# ------------------------------------------------------------\n",
    "df_sql.to_csv(\"results_sql.csv\", index=False, sep=\";\")\n",
    "df_xss.to_csv(\"results_xss.csv\", index=False, sep=\";\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Excel-Workbook mit zwei Sheets\n",
    "# ------------------------------------------------------------\n",
    "with pd.ExcelWriter(\"results_by_dataset.xlsx\") as writer:\n",
    "    df_sql.to_excel(writer, sheet_name=\"SQL\", index=False)\n",
    "    df_xss.to_excel(writer, sheet_name=\"XSS\", index=False)\n",
    "\n",
    "print( \"Dateien wurden exportiert: results_sql.csv, results_xss.csv, results_by_dataset.xlsx\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824c2ae6",
   "metadata": {},
   "source": [
    "# Hyperparameter-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605d0571",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1bd3b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schwellenwert-Tuning (max F1) für Modelle mit predict_proba\n",
    "def tune_threshold(estimator, X_val, y_val, grid=None):\n",
    "    if not hasattr(estimator, \"predict_proba\"):\n",
    "        return 0.5, None  # kein Proba -> Standard-Threshold\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.1, 0.9, 81)\n",
    "    p = estimator.predict_proba(X_val)[:, 1]\n",
    "    best_t, best_f1 = 0.5, -1\n",
    "    for t in grid:\n",
    "        y_hat = (p >= t).astype(int)\n",
    "        f1 = f1_score(y_val, y_hat)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    return best_t, best_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3209ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit und Threshold und Testmetriken\n",
    "def eval_with_threshold(name, est, X_train, y_train, X_test, y_test, do_threshold=True):\n",
    "    # kleine Validierung aus dem Train für Thresholdsuche abspalten (leak-frei)\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.15,\n",
    "                                                stratify=y_train, random_state=RANDOM_STATE)\n",
    "    t0 = time.perf_counter()\n",
    "    est.fit(X_tr, y_tr)\n",
    "    fit_s = time.perf_counter() - t0\n",
    "\n",
    "    thr, f1_val = (0.5, None)\n",
    "    if do_threshold:\n",
    "        thr, f1_val = tune_threshold(est, X_val, y_val)\n",
    "\n",
    "    # Test\n",
    "    t0 = time.perf_counter()\n",
    "    if hasattr(est, \"predict_proba\"):\n",
    "        y_pred = (est.predict_proba(X_test)[:, 1] >= thr).astype(int)\n",
    "    else:\n",
    "        y_pred = est.predict(X_test)\n",
    "    pred_s = time.perf_counter() - t0\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\", zero_division=0)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    fpr = float(fp / (fp + tn)) if (fp + tn) else 0.0\n",
    "    fnr = float(fn / (fn + tp)) if (fn + tp) else 0.0\n",
    "\n",
    "    return dict(Model=name, Fit_s=fit_s, Pred_s=pred_s, Thr=thr, ValF1=f1_val,\n",
    "                Precision=p, Recall=r, F1=f1, FPR=fpr, FNR=fnr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dede8f42",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b1ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "# Einheitlicher TF-IDF\n",
    "def make_tfidf():\n",
    "    return TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5),\n",
    "                           max_features=50_000, sublinear_tf=True, lowercase=False)\n",
    "\n",
    "def pipe_rf():\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", make_tfidf()),\n",
    "        (\"clf\", RandomForestClassifier(n_jobs=-1, random_state=RANDOM_STATE))\n",
    "    ])\n",
    "def pipe_mlp():\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", make_tfidf()),\n",
    "        (\"clf\", MLPClassifier(early_stopping=True, n_iter_no_change=5, max_iter=200,\n",
    "                              random_state=RANDOM_STATE))\n",
    "    ])\n",
    "\n",
    "def pipe_xgb():\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", make_tfidf()),\n",
    "        (\"clf\", XGBClassifier(\n",
    "            tree_method=\"hist\", device=(\"cuda\" if USE_CUDA else \"cpu\"),\n",
    "            random_state=RANDOM_STATE, n_jobs=1  # parallelism über CV, nicht im Estimator\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "def pipe_cat():\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", make_tfidf()),\n",
    "        (\"clf\", CatBoostClassifier(\n",
    "            loss_function=\"Logloss\", verbose=False, random_seed=RANDOM_STATE\n",
    "            # task_type=\"GPU\" geht auch, aber dann Dense-Arrays besser\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "# TabPFN: TF-IDF -> SVD -> TabPFN (nur SVD-Dim tunen; TabPFN selbst kaum Hyperparams)\n",
    "def pipe_tabpfn(n_components=150):\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", make_tfidf()),\n",
    "        (\"svd\", TruncatedSVD(n_components=n_components, random_state=RANDOM_STATE)),\n",
    "        (\"clf\", TabPFNClassifier(device=(\"cuda\" if USE_CUDA else \"cpu\"),\n",
    "                                 ignore_pretraining_limits=True))\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a37757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_weight(y):\n",
    "    # für XGB: scale_pos_weight ~ Neg/Pos\n",
    "    pos = max(1, int(np.sum(y)))\n",
    "    neg = max(1, int(len(y)-pos))\n",
    "    return neg / pos\n",
    "\n",
    "param_grids = {\n",
    "    \"RandomForest\": {\n",
    "        \"tfidf__max_features\": [30_000, 50_000, 80_000],\n",
    "        \"clf__n_estimators\": [300, 500, 800],\n",
    "        \"clf__max_depth\": [None, 20, 40, 80],\n",
    "        \"clf__max_features\": [\"sqrt\", 0.5, None],\n",
    "        \"clf__min_samples_split\": [2, 5, 10],\n",
    "        \"clf__class_weight\": [None, \"balanced\"]\n",
    "    },\n",
    "    \"MLP\": {\n",
    "        \"tfidf__max_features\": [30_000, 50_000],\n",
    "        \"clf__hidden_layer_sizes\": [(256,128), (512,256), (256,)],\n",
    "        \"clf__alpha\": np.logspace(-5, -2, 4),\n",
    "        \"clf__learning_rate_init\": np.logspace(-4, -2, 3),\n",
    "        \"clf__batch_size\": [64, 128, 256]\n",
    "    },\n",
    "    \"XGBoost\": lambda y: {\n",
    "        \"tfidf__max_features\": [30_000, 50_000],\n",
    "        \"clf__n_estimators\": [300, 500, 800],\n",
    "        \"clf__max_depth\": [4, 6, 8],\n",
    "        \"clf__learning_rate\": [0.05, 0.1, 0.2],\n",
    "        \"clf__subsample\": [0.7, 0.9, 1.0],\n",
    "        \"clf__colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "        \"clf__reg_lambda\": [0, 1, 5, 10],\n",
    "        \"clf__gamma\": [0, 1, 2],\n",
    "        \"clf__scale_pos_weight\": [pos_weight(y)]\n",
    "    },\n",
    "    \"CatBoost\": {\n",
    "        \"tfidf__max_features\": [30_000, 50_000],\n",
    "        \"clf__iterations\": [400, 800, 1000],\n",
    "        \"clf__depth\": [6, 8, 10],\n",
    "        \"clf__learning_rate\": [0.05, 0.1, 0.2],\n",
    "        \"clf__l2_leaf_reg\": [1, 3, 5, 10]\n",
    "    },\n",
    "# TabPFN: lieber nur SVD-Dimension suchen (sonst sehr langsam)\n",
    "    \"TabPFN\": {\n",
    "        \"tfidf__max_features\": [30_000, 50_000],\n",
    "        \"svd__n_components\": [100, 150, 200]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfd68bf",
   "metadata": {},
   "source": [
    "## Tuning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41785eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def tune_model(name, X_text, y, cv_splits=3, n_iter=20, random_state=RANDOM_STATE, run_tabpfn=False):\n",
    "    print(f\"\\n=== TUNING: {name} ===\")\n",
    "    if name == \"RandomForest\":\n",
    "        pipe = pipe_rf(); space = param_grids[\"RandomForest\"]\n",
    "    elif name == \"MLP\":\n",
    "        pipe = pipe_mlp(); space = param_grids[\"MLP\"]\n",
    "    elif name == \"XGBoost\":\n",
    "        pipe = pipe_xgb(); space = param_grids[\"XGBoost\"](y)\n",
    "    elif name == \"CatBoost\":\n",
    "        pipe = pipe_cat(); space = param_grids[\"CatBoost\"]\n",
    "    elif name == \"TabPFN\":\n",
    "        if not run_tabpfn:\n",
    "            print(\"Skipping TabPFN tuning (set run_tabpfn=True to enable).\")\n",
    "            return None\n",
    "        pipe = pipe_tabpfn()\n",
    "        space = param_grids[\"TabPFN\"]\n",
    "        # TabPFN: reduziere CV-Last\n",
    "        cv_splits = 2\n",
    "        n_iter = min(n_iter, 6)\n",
    "    else:\n",
    "        raise ValueError(name)\n",
    "\n",
    "    scorer = make_scorer(f1_score)\n",
    "    cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=random_state)\n",
    "    # XGB (GPU) parallelisiert schlecht über Prozesse -> n_jobs=1; sonst gerne -1\n",
    "    search_n_jobs = 1 if name == \"XGBoost\" else -1\n",
    "\n",
    "    rs = RandomizedSearchCV(\n",
    "        pipe, space, n_iter=n_iter, scoring=scorer, cv=cv,\n",
    "        random_state=random_state, n_jobs=search_n_jobs, verbose=1\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    rs.fit(X_text, y)\n",
    "    fit_s = time.perf_counter() - t0\n",
    "    print(f\"[{name}] best F1 (CV): {rs.best_score_:.4f} | best params: {rs.best_params_} | time: {fit_s/60:.1f} min\")\n",
    "    return rs.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1174ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tuning_for_dataset(df, ds_name, run_tabpfn=False):\n",
    "    # Labels + Rohtext (deine Clean-Funktion)\n",
    "    X_raw, y = clean_labels(df, label_col=\"Label\", text_cols=(\"Sentence\",))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_raw, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    best_models = {}\n",
    "\n",
    "    for name in [\"RandomForest\", \"MLP\", \"XGBoost\", \"CatBoost\"]:  # TabPFN separat\n",
    "        best = tune_model(name, X_train, y_train, n_iter=20)\n",
    "        best_models[name] = best\n",
    "        res = eval_with_threshold(f\"{name}-{ds_name}\", best, X_train, y_train, X_test, y_test, do_threshold=True)\n",
    "        print(res); results.append(res)\n",
    "\n",
    "    # Optional: TabPFN sehr vorsichtig\n",
    "    if run_tabpfn:\n",
    "        best = tune_model(\"TabPFN\", X_train, y_train, run_tabpfn=True)\n",
    "        if best is not None:\n",
    "            # Für TabPFN am Ende keine großen Batches nötig, Pipeline macht SVD intern.\n",
    "            res = eval_with_threshold(f\"TabPFN-{ds_name}\", best, X_train, y_train, X_test, y_test, do_threshold=False)\n",
    "            print(res); results.append(res)\n",
    "\n",
    "    return pd.DataFrame(results), best_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d496b92c",
   "metadata": {},
   "source": [
    "## Run & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d3c9172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TUNING: RandomForest ===\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_sql_res, sql_models = \u001b[43mrun_tuning_for_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSQL\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_tabpfn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# TabPFN erstmal aus\u001b[39;00m\n\u001b[32m      2\u001b[39m df_xss_res, xss_models = run_tuning_for_dataset(xss_df, \u001b[33m\"\u001b[39m\u001b[33mXSS\u001b[39m\u001b[33m\"\u001b[39m, run_tabpfn=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      4\u001b[39m df_all = pd.concat([df_sql_res, df_xss_res], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mrun_tuning_for_dataset\u001b[39m\u001b[34m(df, ds_name, run_tabpfn)\u001b[39m\n\u001b[32m      9\u001b[39m best_models = {}\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mRandomForest\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMLP\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mXGBoost\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCatBoost\u001b[39m\u001b[33m\"\u001b[39m]:  \u001b[38;5;66;03m# TabPFN separat\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     best = \u001b[43mtune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     best_models[name] = best\n\u001b[32m     14\u001b[39m     res = eval_with_threshold(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, best, X_train, y_train, X_test, y_test, do_threshold=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mtune_model\u001b[39m\u001b[34m(name, X_text, y, cv_splits, n_iter, random_state, run_tabpfn)\u001b[39m\n\u001b[32m     33\u001b[39m rs = RandomizedSearchCV(\n\u001b[32m     34\u001b[39m     pipe, space, n_iter=n_iter, scoring=scorer, cv=cv,\n\u001b[32m     35\u001b[39m     random_state=random_state, n_jobs=search_n_jobs, verbose=\u001b[32m1\u001b[39m\n\u001b[32m     36\u001b[39m )\n\u001b[32m     37\u001b[39m t0 = time.perf_counter()\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m fit_s = time.perf_counter() - t0\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] best F1 (CV): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrs.best_score_\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | best params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrs.best_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfit_s/\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m min\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nilsp\\Github_Desktop\\Comparative_Study_ML_WebVuln\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nilsp\\Github_Desktop\\Comparative_Study_ML_WebVuln\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nilsp\\Github_Desktop\\Comparative_Study_ML_WebVuln\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1951\u001b[39m, in \u001b[36mRandomizedSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1950\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1953\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m   1954\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1955\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nilsp\\Github_Desktop\\Comparative_Study_ML_WebVuln\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nilsp\\Github_Desktop\\Comparative_Study_ML_WebVuln\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nilsp\\Github_Desktop\\Comparative_Study_ML_WebVuln\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nilsp\\Github_Desktop\\Comparative_Study_ML_WebVuln\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nilsp\\Github_Desktop\\Comparative_Study_ML_WebVuln\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "df_sql_res, sql_models = run_tuning_for_dataset(sql_df, \"SQL\", run_tabpfn=False)   # TabPFN erstmal aus\n",
    "df_xss_res, xss_models = run_tuning_for_dataset(xss_df, \"XSS\", run_tabpfn=False)\n",
    "\n",
    "df_all = pd.concat([df_sql_res, df_xss_res], ignore_index=True)\n",
    "display(df_all)\n",
    "\n",
    "# Excel-Export\n",
    "out = Path(\"reports\") / \"tuning_results.xlsx\"\n",
    "out.parent.mkdir(exist_ok=True, parents=True)\n",
    "with pd.ExcelWriter(out) as w:\n",
    "    df_all.to_excel(w, sheet_name=\"All\", index=False)\n",
    "    df_sql_res.to_excel(w, sheet_name=\"SQL\", index=False)\n",
    "    df_xss_res.to_excel(w, sheet_name=\"XSS\", index=False)\n",
    "print(f\"Saved:{out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd036337",
   "metadata": {},
   "source": [
    "## K-Fold-Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ae0362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
