{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41cdae0",
   "metadata": {},
   "source": [
    "# Code zu der Bachelorarbeit:\n",
    "# \"Comparitve Study von Machine Learning Modellen zur Erkennung von Web Schwachstellen\"\n",
    "## von Nils Pudenz, 2735230"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21512b38",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dac7f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%pip install kaggle scikit-learn xgboost catboost tabpfn pandas numpy matplotlib seaborn -q\n",
    "#%pip install --quiet scikit-learn xgboost catboost tabpfn chardet\n",
    "#%pip install -U scikit-learn\n",
    "## in deiner (Conda/venv) Umgebung\n",
    "#%pip install --upgrade \"torch==2.*\" --index-url https://download.pytorch.org/whl/cu121\n",
    "#%pip install --upgrade xgboost catboost scikit-learn pandas scipy tabpfn\n",
    "##wenn es komplikationen mit torch gibt, deiinstallieren und neu installieren\n",
    "#%pip uninstall torch\n",
    "#%pip install torch --index-url https://download.pytorch.org/whl/cu121 --upgrade\n",
    "#%pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffe89a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import zipfile\n",
    "import random\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tabpfn import TabPFNClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils import resample\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915add7c",
   "metadata": {},
   "source": [
    "## Check ob GPU verwendet werden kann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5103769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n",
      "CUDA verfügbar: True\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)\n",
    "print(\"CUDA verfügbar:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9b9e350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: c:\\Users\\nilsp\\Github_Desktop\\Comparative_Study_ML_WebVuln\\.venv\\Scripts\\python.exe\n",
      "Torch: 2.5.1+cu121 | CUDA build: 12.1\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Python:\", sys.executable)\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA build:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4fea7e",
   "metadata": {},
   "source": [
    "## mögliche Modeloptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47f16504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nxgb = XGBClassifier(\\n    tree_method=\"hist\",        # CPU-Optimierung\\n    n_estimators=400,\\n    max_depth=6,\\n    learning_rate=0.1,\\n    subsample=0.9,\\n    colsample_bytree=0.8,\\n    n_jobs=-1,\\n    random_state=42\\n)\\n\\nfrom catboost import CatBoostClassifier\\ncat = CatBoostClassifier(\\n    iterations=400,\\n    depth=8,\\n    learning_rate=0.1,\\n    random_seed=42,\\n    loss_function=\"Logloss\",\\n    task_type=\"CPU\",\\n    thread_count=8,\\n    od_type=\"Iter\",            # early stopping\\n    od_wait=30,\\n    verbose=False\\n)\\n#'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "xgb = XGBClassifier(\n",
    "    tree_method=\"hist\",        # CPU-Optimierung\n",
    "    n_estimators=400,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.8,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "cat = CatBoostClassifier(\n",
    "    iterations=400,\n",
    "    depth=8,\n",
    "    learning_rate=0.1,\n",
    "    random_seed=42,\n",
    "    loss_function=\"Logloss\",\n",
    "    task_type=\"CPU\",\n",
    "    thread_count=8,\n",
    "    od_type=\"Iter\",            # early stopping\n",
    "    od_wait=30,\n",
    "    verbose=False\n",
    ")\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "407c50e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvec = TfidfVectorizer(\\n    ngram_range=(3,5),\\n    max_features=50_000,\\n    sublinear_tf=True,\\n    lowercase=False\\n).fit(X_train_raw)           # nur einmal fitten\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Speicher optimieren\n",
    "'''\n",
    "vec = TfidfVectorizer(\n",
    "    ngram_range=(3,5),\n",
    "    max_features=50_000,\n",
    "    sublinear_tf=True,\n",
    "    lowercase=False\n",
    ").fit(X_train_raw)           # nur einmal fitten\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fb31025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsearch_space = {\\n    \"max_depth\": [4, 6, 8],\\n    \"learning_rate\": [0.05, 0.1, 0.2],\\n    \"n_estimators\": [200, 400, 600]\\n}\\nrandcv = RandomizedSearchCV(\\n    xgb,\\n    search_space,\\n    n_iter=10,            # statt 3×3×3 = 27\\n    scoring=\"f1\",\\n    cv=3,\\n    n_jobs=-1\\n)\\nrandcv.fit(X_train_vec, y_train)\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#kleine Hyperparamsuche statt Grid-Overkill\n",
    "'''\n",
    "search_space = {\n",
    "    \"max_depth\": [4, 6, 8],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "    \"n_estimators\": [200, 400, 600]\n",
    "}\n",
    "randcv = RandomizedSearchCV(\n",
    "    xgb,\n",
    "    search_space,\n",
    "    n_iter=10,            # statt 3×3×3 = 27\n",
    "    scoring=\"f1\",\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "randcv.fit(X_train_vec, y_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7ba5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d283a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_in_batches(model, X, batch_size=512): #um Überlastung zu vermeiden, evtl. 256 oder 128\n",
    "    \"\"\"Make predictions on input data in batches.\"\"\"\n",
    "    preds = []\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        batch = X[i:i + batch_size]\n",
    "        preds.append(model.predict(batch))\n",
    "    return np.concatenate(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0350a6b7",
   "metadata": {},
   "source": [
    "## Dowload Kaggle Datasets\n",
    "Requires Kaggle API credentials ('~/.kaggle/kaggle.json') für API-Token, um zugriff auf die Datenbanken über das Kaggle Konto zu bekommen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "749404d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e33f5a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dowload der Datasets von Kaggle, Output =1 erfoglgreich, Output = 0 fehlerhaft\n",
    "os.system(\"kaggle datasets download -d syedsaqlainhussain/sql-injection-dataset -p data --unzip --quiet\")\n",
    "os.system(\"kaggle datasets download -d syedsaqlainhussain/cross-site-scripting-xss-dataset-for-deep-learning -p data --unzip --quiet\")\n",
    "#KAGGLE_DATASETS = { #gleich wie oben nur renaming auf sql & xss\n",
    "#    \"sql\": \"syedsaqlainhussain/sql-injection-dataset\",\n",
    "#    \"xss\": \"syedsaqlainhussain/cross-site-scripting-xss-dataset-for-deep-learning\"\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4fe4a3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/syedsaqlainhussain/sql-injection-dataset\n",
      "Downloaded syedsaqlainhussain/sql-injection-dataset\n",
      "Dataset URL: https://www.kaggle.com/datasets/syedsaqlainhussain/cross-site-scripting-xss-dataset-for-deep-learning\n",
      "Downloaded syedsaqlainhussain/cross-site-scripting-xss-dataset-for-deep-learning\n"
     ]
    }
   ],
   "source": [
    "def kaggle_download(dataset, path=\"data\", unzip=True):\n",
    "    api = KaggleApi()\n",
    "    api.authenticate() #nutzt ~/.kaggle/kaggle.json für Authentifizierung oder Environment-Variablen\n",
    "    api.dataset_download_files(dataset, path=path, unzip=unzip)\n",
    "    print(f\"Downloaded {dataset}\")\n",
    "\n",
    "kaggle_download(\"syedsaqlainhussain/sql-injection-dataset\")#, path=DATA_DIR, unzip=True)\n",
    "kaggle_download(\"syedsaqlainhussain/cross-site-scripting-xss-dataset-for-deep-learning\")#, path=DATA_DIR, unzip=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51b9480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sql_df = pd.read_csv(\"data/SQLiV3.csv\", encoding=\"utf-8\", low_memory=False)\n",
    "xss_df = pd.read_csv(\"data/XSS_dataset.csv\", encoding=\"utf-8\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d3a3440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentence', 'Label', 'Unnamed: 2', 'Unnamed: 3']\n",
      "clean shape: (30873, 2)\n",
      "Label\n",
      "0                                                                         0.628891\n",
      "1                                                                         0.370162\n",
      " --                                                                       0.000359\n",
      "waitfor delay '0:0:__TIME__'--                                            0.000131\n",
      " DROP TABLE Suppliers                                                     0.000065\n",
      " desc users                                                               0.000033\n",
      "SELECT *                                                                  0.000033\n",
      " OR                                                                       0.000033\n",
      " if not  (  select system_user  )   <> 'sa' waitfor delay '0:0:2' --      0.000033\n",
      " drop table temp --                                                       0.000033\n",
      " grant resource to name                                                   0.000033\n",
      " /*Select all the columns of all the records in the Customers table:*/    0.000033\n",
      " EXEC SelectAllCustomers                                                  0.000033\n",
      "*/                                                                        0.000033\n",
      " CREATE VIEW [Products Above Average Price] AS                            0.000033\n",
      " CREATE OR REPLACE VIEW view_name AS                                      0.000033\n",
      " SELECT column_name ( s )                                                 0.000033\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Spalten ansehen\n",
    "print(sql_df.columns.tolist())\n",
    "\n",
    "# Typische Index-/Hilfsspalten loswerden\n",
    "sql_df = sql_df.loc[:, ~sql_df.columns.str.contains(r\"^Unnamed|^index$\", case=False)]\n",
    "\n",
    "# Auf die Kernspalten reduzieren (falls etwas anderes drin ist)\n",
    "sql_df = sql_df[[\"Sentence\", \"Label\"]].copy()\n",
    "\n",
    "# Optional: Duplikate auf Satzebene entfernen (falls noch nicht passiert)\n",
    "sql_df = sql_df.drop_duplicates(subset=[\"Sentence\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"clean shape:\", sql_df.shape)  # Erwartung: (30873, 2)\n",
    "print(sql_df[\"Label\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36a79aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL dataset shape: (30873, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\" or pg_sleep  (  __TIME__  )  --</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>create user name identified by pass123 tempora...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AND 1  =  utl_inaddr.get_host_address   (    ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>select * from users where id  =  '1' or @ @1 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>select * from users where id  =  1 or 1#\"  ( ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Label\n",
       "0                  \" or pg_sleep  (  __TIME__  )  --     1\n",
       "1  create user name identified by pass123 tempora...   NaN\n",
       "2   AND 1  =  utl_inaddr.get_host_address   (    ...     1\n",
       "3   select * from users where id  =  '1' or @ @1 ...     1\n",
       "4   select * from users where id  =  1 or 1#\"  ( ...     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30872</td>\n",
       "      <td>30619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>30872</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>\" or pg_sleep  (  __TIME__  )  --</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>19256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Sentence  Label\n",
       "count                               30872  30619\n",
       "unique                              30872     17\n",
       "top     \" or pg_sleep  (  __TIME__  )  --      0\n",
       "freq                                    1  19256"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30873 entries, 0 to 30872\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sentence  30872 non-null  object\n",
      " 1   Label     30619 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 482.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XSS dataset shape: (13686, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;li&gt;&lt;a href=\"/wiki/File:Socrates.png\" class=\"i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;tt onmouseover=\"alert(1)\"&gt;test&lt;/tt&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\t &lt;/span&gt; &lt;span class=\"reference-text\"&gt;Steeri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\t &lt;/span&gt; &lt;span class=\"reference-text\"&gt;&lt;cite ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\\t &lt;/span&gt;. &lt;a href=\"/wiki/Digital_object_iden...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           Sentence  Label\n",
       "0           0  <li><a href=\"/wiki/File:Socrates.png\" class=\"i...      0\n",
       "1           1               <tt onmouseover=\"alert(1)\">test</tt>      1\n",
       "2           2  \\t </span> <span class=\"reference-text\">Steeri...      0\n",
       "3           3  \\t </span> <span class=\"reference-text\"><cite ...      0\n",
       "4           4  \\t </span>. <a href=\"/wiki/Digital_object_iden...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13686.000000</td>\n",
       "      <td>13686.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6842.500000</td>\n",
       "      <td>0.538726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3950.952227</td>\n",
       "      <td>0.498516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3421.250000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6842.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10263.750000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13685.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0         Label\n",
       "count  13686.000000  13686.000000\n",
       "mean    6842.500000      0.538726\n",
       "std     3950.952227      0.498516\n",
       "min        0.000000      0.000000\n",
       "25%     3421.250000      0.000000\n",
       "50%     6842.500000      1.000000\n",
       "75%    10263.750000      1.000000\n",
       "max    13685.000000      1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13686 entries, 0 to 13685\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  13686 non-null  int64 \n",
      " 1   Sentence    13686 non-null  object\n",
      " 2   Label       13686 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 320.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for name, df in {\"SQL\": sql_df, \"XSS\": xss_df}.items():\n",
    "    print(f\"{name} dataset shape: {df.shape}\")\n",
    "    display(df.head())\n",
    "    display(df.describe())\n",
    "    display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1d30aa",
   "metadata": {},
   "source": [
    "## Basic Cleaning\n",
    "* Drop Duplicate rows\n",
    "* Handle missing values (simple fill-na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce598302",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (sql_df, xss_df):\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5eedf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_xy(df: pd.DataFrame,\n",
    "                  label_candidates=(\"label\", \"class\", \"target\"),\n",
    "                  label_map=None):\n",
    "    if label_map is None:\n",
    "        label_map = {\n",
    "            \"0\": \"0\", \"1\": \"1\",\n",
    "            \"benign\": \"0\", \"normal\": \"0\", \"legitimate\": \"0\", \"safe\": \"0\",\n",
    "            \"attack\": \"1\", \"malicious\": \"1\", \"sql injection\": \"1\",\n",
    "            \"sql-injection\": \"1\", \"xss\": \"1\"\n",
    "        }\n",
    "\n",
    "    # Zielspalte finden (im *übergebenen* df!)\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    target_col = next((cols_lower[c] for c in label_candidates if c in cols_lower), None)\n",
    "    if target_col is None:\n",
    "        raise ValueError(f\"Keine Label-Spalte gefunden. Kandidaten: {label_candidates}\")\n",
    "\n",
    "    # Labels normieren -> nur 0/1 behalten\n",
    "    y_str = df[target_col].astype(str).str.strip().str.lower()\n",
    "    y_map = y_str.map(label_map)\n",
    "    mask = y_map.notna()\n",
    "    y = pd.to_numeric(y_map[mask]).astype(int).to_numpy()\n",
    "\n",
    "    # Rohtext aus allen Nicht-Label-Spalten zusammenbauen\n",
    "    feat_cols = [c for c in df.columns if c != target_col]\n",
    "    X_raw = df.loc[mask, feat_cols].astype(str).agg(\" \".join, axis=1)\n",
    "\n",
    "    return X_raw, y, target_col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ce09f",
   "metadata": {},
   "source": [
    "# Globale Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6a3f45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ENV] CUDA avail: True | Torch CUDA build: 12.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Deterministische Ausgabe generieren, um die Reproduzierbarkeit zu gewährleisten\n",
    "RANDOM_STATE = 42\n",
    "#np.random.seed(RANDOM_STATE) wofür das?\n",
    "#random.seed(RANDOM_STATE)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"8\"\n",
    "torch.set_num_threads(8)\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()  # für TabPFN & XGBoost\n",
    "print(f\"[ENV] CUDA avail: {USE_CUDA} | Torch CUDA build: {torch.version.cuda}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee6d9eb",
   "metadata": {},
   "source": [
    "# Hilfsfunktionen "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790bf05a",
   "metadata": {},
   "source": [
    "## Evaluationsmetriken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba189305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_metrics(y_true, y_pred) -> dict:\n",
    "    \"\"\"Präzision/Recall/F1 & Raten (FPR/FNR) für binäre Klassifikation.\"\"\"\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    p  = precision_score(y_true, y_pred)\n",
    "    r  = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    fpr = float(fp / (fp + tn)) if (fp + tn) else 0.0\n",
    "    fnr = float(fn / (fn + tp)) if (fn + tp) else 0.0\n",
    "    return dict(Precision=p, Recall=r, F1=f1, FPR=fpr, FNR=fnr)\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, name, use_batches=False, batch_size=256) -> dict: #Dictionary für Evaluierungsmetriken\n",
    "    \"\"\"Zeitmessung + Vorhersage (optional in Batches) + Metriken.\"\"\"\n",
    "    print(f\"→ Evaluate {name} on X_test={getattr(X_test,'shape',None)}\")\n",
    "    t0 = time.perf_counter()\n",
    "    if use_batches:\n",
    "        y_pred = predict_in_batches(model, X_test, batch_size=batch_size, verbose=True)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "    pred_s = time.perf_counter() - t0\n",
    "    m = binary_metrics(y_test, y_pred)\n",
    "    res = dict(Model=name, Pred_s=pred_s, **m)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7c572f",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b10bd60",
   "metadata": {},
   "source": [
    "### Batchweise Vorhersage für TabPFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9fc7885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_in_batches(model, X, batch_size=256, verbose=False):\n",
    "    \"\"\"Vorhersage in Batches (schont RAM/VRAM; wichtig für TabPFN).\"\"\"\n",
    "    n = X.shape[0]\n",
    "    out = []\n",
    "    total = math.ceil(n / batch_size)\n",
    "    for b, i in enumerate(range(0, n, batch_size), start=1):\n",
    "        j = min(i + batch_size, n)\n",
    "        t1 = time.perf_counter()\n",
    "        with torch.inference_mode():\n",
    "            out.append(model.predict(X[i:j]))\n",
    "        dt = time.perf_counter() - t1\n",
    "        if verbose:\n",
    "            print(f\"   [predict] batch {b:>3}/{total} ({j-i} rows) in {dt:.2f}s\")\n",
    "        if USE_CUDA:\n",
    "            torch.cuda.synchronize()\n",
    "    return np.concatenate(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2e87bd",
   "metadata": {},
   "source": [
    "### Label bereinigung der Datensets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8bbb5358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_labels(df, label_col=\"Label\", text_cols=(\"Sentence\",)):\n",
    "    \"\"\"Bringt Labels robust auf {0,1} und gibt (X_raw, y) zurück.\"\"\"\n",
    "    df = df.copy()\n",
    "    y_raw = df[label_col].astype(str).str.strip().str.lower()\n",
    "    map01 = {\"0\": \"0\", \"1\": \"1\", \"benign\": \"0\", \"normal\": \"0\", \"legitimate\": \"0\", \"safe\": \"0\",\n",
    "             \"attack\": \"1\", \"malicious\": \"1\", \"sql injection\": \"1\", \"sql-injection\": \"1\", \"xss\": \"1\"}\n",
    "    y_map = y_raw.map(map01)\n",
    "    mask = y_map.notna()\n",
    "    y = pd.to_numeric(y_map[mask]).astype(int).to_numpy()\n",
    "    feat_cols = [c for c in df.columns if c != label_col]\n",
    "    X_raw = df.loc[mask, feat_cols].astype(str).agg(\" \".join, axis=1)\n",
    "    return X_raw, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875e8afa",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17e9bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Falls clean_labels(X) bereits (Rohtext, y) zurückgibt:\n",
    "X_txt_sql, y_sql = clean_labels(sql_df, label_col=\"Label\", text_cols=(\"Sentence\",))\n",
    "X_train_sql, X_test_sql, y_train_sql, y_test_sql = train_test_split(\n",
    "    X_txt_sql, y_sql, test_size=0.2, stratify=y_sql, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_txt_xss, y_xss = clean_labels(xss_df, label_col=\"Label\", text_cols=(\"Sentence\",))\n",
    "X_train_xss, X_test_xss, y_train_xss, y_test_xss = train_test_split(\n",
    "    X_txt_xss, y_xss, test_size=0.2, stratify=y_xss, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe7b6ec",
   "metadata": {},
   "source": [
    "# Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18928fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=RANDOM_STATE),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(256,128), activation=\"relu\",\n",
    "                         early_stopping=True, n_iter_no_change=5, max_iter=200,\n",
    "                         random_state=RANDOM_STATE),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=500, max_depth=6, learning_rate=0.1,\n",
    "        subsample=0.9, colsample_bytree=0.8,\n",
    "        tree_method=\"hist\", device=(\"cuda\" if USE_CUDA else \"cpu\"),\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "        iterations=400, depth=8, learning_rate=0.1,\n",
    "        loss_function=\"Logloss\", random_seed=RANDOM_STATE, verbose=False,\n",
    "        task_type=\"CPU\"   # stabil über Pool + Sparse; bei GPU: task_type=\"GPU\" und ggf. Dense verwenden\n",
    "    ),\n",
    "    \"TabPFN\": TabPFNClassifier(\n",
    "        device=(\"cuda\" if USE_CUDA else \"cpu\"),\n",
    "        ignore_pretraining_limits=True\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690192f4",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ee0af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TDF-IDF Einstellungen\n",
    "VEC_ARGS = dict(ngram_range=(3,5), max_features=50_000, sublinear_tf=True, lowercase=False)\n",
    "#VEC_ARGS = dict(analyzer=\"char\", ngram_range=(3,5), min_df=3, max_features=50_000, sublinear_tf=True, lowercase=False, dtype=np.float32)\n",
    "\n",
    "\n",
    "TABPFN_MAX_SAMPLES = 4000     # starte konservativ; später 6000/8000 testen\n",
    "TABPFN_N_COMPONENTS = 150     # <=500, kleiner = schneller/ram-sparender\n",
    "TABPFN_BATCH = 128            # Batch für Predict; 128/64 bei RAM-Engpässen\n",
    "\n",
    "results = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3c27db",
   "metadata": {},
   "source": [
    "## Pipeline je Datensatz (TF-IDF; TabPFN: SVD + batched predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "59857f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATASET: SQL | train=24675 test=6169 (pos_rate_train=0.367)\n",
      "[VEC] TF-IDF train=(24675, 50000), test=(6169, 50000) in 1.11s\n",
      "--------------------------------------------------\n",
      "MODEL: RandomForest on SQL\n",
      "[RandomForest] fit in 14.05s\n",
      "→ Evaluate RandomForest-SQL on X_test=(6169, 50000)\n",
      "{'Model': 'RandomForest-SQL', 'Pred_s': 0.8627100996673107, 'Precision': 0.9917172832689122, 'Recall': 0.792236435818262, 'F1': 0.8808239333006376, 'FPR': 0.0038441824705279346, 'FNR': 0.20776356418173797}\n",
      "--------------------------------------------------\n",
      "MODEL: MLP on SQL\n",
      "[MLP] fit in 812.74s\n",
      "→ Evaluate MLP-SQL on X_test=(6169, 50000)\n",
      "{'Model': 'MLP-SQL', 'Pred_s': 0.02129790000617504, 'Precision': 0.9911894273127754, 'Recall': 0.7940008822232024, 'F1': 0.881704628949302, 'FPR': 0.004100461301896463, 'FNR': 0.20599911777679752}\n",
      "--------------------------------------------------\n",
      "MODEL: XGBoost on SQL\n",
      "[XGBoost] fit in 10.33s\n",
      "→ Evaluate XGBoost-SQL on X_test=(6169, 50000)\n",
      "{'Model': 'XGBoost-SQL', 'Pred_s': 0.04113319981843233, 'Precision': 1.0, 'Recall': 0.4239082487869431, 'F1': 0.5954151177199505, 'FPR': 0.0, 'FNR': 0.5760917512130569}\n",
      "--------------------------------------------------\n",
      "MODEL: CatBoost on SQL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nilsp\\Github_Desktop\\Comparative_Study_ML_WebVuln\\.venv\\Lib\\site-packages\\xgboost\\core.py:729: UserWarning: [17:36:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CatBoost] fit in 136.01s\n",
      "→ Evaluate CatBoost-SQL on X_test=(6169, 50000)\n",
      "{'Model': 'CatBoost-SQL', 'Pred_s': 0.13463389966636896, 'Precision': 1.0, 'Recall': 0.7878253198059109, 'F1': 0.8813224771773994, 'FPR': 0.0, 'FNR': 0.2121746801940891}\n",
      "--------------------------------------------------\n",
      "MODEL: TabPFN on SQL\n",
      "[TabPFN] train subset: 4000 rows\n",
      "[TabPFN] SVD train=(4000, 150), test=(6169, 150) in 3.05s\n",
      "[TabPFN] fit in 68.69s (device=cuda)\n",
      "→ Evaluate TabPFN-SQL on X_test=(6169, 150)\n",
      "   [predict] batch   1/49 (128 rows) in 27.49s\n",
      "   [predict] batch   2/49 (128 rows) in 33.68s\n",
      "   [predict] batch   3/49 (128 rows) in 51.46s\n",
      "   [predict] batch   4/49 (128 rows) in 51.10s\n",
      "   [predict] batch   5/49 (128 rows) in 52.48s\n",
      "   [predict] batch   6/49 (128 rows) in 51.37s\n",
      "   [predict] batch   7/49 (128 rows) in 49.35s\n",
      "   [predict] batch   8/49 (128 rows) in 698.13s\n",
      "   [predict] batch   9/49 (128 rows) in 262.15s\n",
      "   [predict] batch  10/49 (128 rows) in 48.11s\n",
      "   [predict] batch  11/49 (128 rows) in 47.34s\n",
      "   [predict] batch  12/49 (128 rows) in 47.59s\n",
      "   [predict] batch  13/49 (128 rows) in 178.87s\n",
      "   [predict] batch  14/49 (128 rows) in 257.00s\n",
      "   [predict] batch  15/49 (128 rows) in 230.74s\n",
      "   [predict] batch  16/49 (128 rows) in 261.86s\n",
      "   [predict] batch  17/49 (128 rows) in 280.72s\n",
      "   [predict] batch  18/49 (128 rows) in 183.86s\n",
      "   [predict] batch  19/49 (128 rows) in 122.02s\n",
      "   [predict] batch  20/49 (128 rows) in 110.44s\n",
      "   [predict] batch  21/49 (128 rows) in 172.65s\n",
      "   [predict] batch  22/49 (128 rows) in 106.63s\n",
      "   [predict] batch  23/49 (128 rows) in 96.58s\n",
      "   [predict] batch  24/49 (128 rows) in 57.67s\n",
      "   [predict] batch  25/49 (128 rows) in 58.30s\n",
      "   [predict] batch  26/49 (128 rows) in 61.81s\n",
      "   [predict] batch  27/49 (128 rows) in 59.00s\n",
      "   [predict] batch  28/49 (128 rows) in 61.09s\n",
      "   [predict] batch  29/49 (128 rows) in 58.77s\n",
      "   [predict] batch  30/49 (128 rows) in 58.70s\n",
      "   [predict] batch  31/49 (128 rows) in 58.12s\n",
      "   [predict] batch  32/49 (128 rows) in 58.71s\n",
      "   [predict] batch  33/49 (128 rows) in 58.36s\n",
      "   [predict] batch  34/49 (128 rows) in 58.19s\n",
      "   [predict] batch  35/49 (128 rows) in 58.31s\n",
      "   [predict] batch  36/49 (128 rows) in 58.70s\n",
      "   [predict] batch  37/49 (128 rows) in 58.78s\n",
      "   [predict] batch  38/49 (128 rows) in 57.94s\n",
      "   [predict] batch  39/49 (128 rows) in 54.81s\n",
      "   [predict] batch  40/49 (128 rows) in 55.41s\n",
      "   [predict] batch  41/49 (128 rows) in 54.80s\n",
      "   [predict] batch  42/49 (128 rows) in 54.79s\n",
      "   [predict] batch  43/49 (128 rows) in 54.21s\n",
      "   [predict] batch  44/49 (128 rows) in 54.30s\n",
      "   [predict] batch  45/49 (128 rows) in 56.43s\n",
      "   [predict] batch  46/49 (128 rows) in 90.34s\n",
      "   [predict] batch  47/49 (128 rows) in 56.06s\n",
      "   [predict] batch  48/49 (128 rows) in 53.41s\n",
      "   [predict] batch  49/49 (25 rows) in 167.38s\n",
      "{'Model': 'TabPFN-SQL', 'Pred_s': 5056.038699799683, 'Precision': 1.0, 'Recall': 0.7838553153947949, 'F1': 0.8788328387734916, 'FPR': 0.0, 'FNR': 0.2161446846052051}\n",
      "\n",
      "======================================================================\n",
      "DATASET: XSS | train=10948 test=2738 (pos_rate_train=0.539)\n",
      "[VEC] TF-IDF train=(10948, 50000), test=(2738, 50000) in 1.08s\n",
      "--------------------------------------------------\n",
      "MODEL: RandomForest on XSS\n",
      "[RandomForest] fit in 1.52s\n",
      "→ Evaluate RandomForest-XSS on X_test=(2738, 50000)\n",
      "{'Model': 'RandomForest-XSS', 'Pred_s': 0.09035099996253848, 'Precision': 1.0, 'Recall': 0.9450847457627118, 'F1': 0.9717671662600209, 'FPR': 0.0, 'FNR': 0.05491525423728814}\n",
      "--------------------------------------------------\n",
      "MODEL: MLP on XSS\n",
      "[MLP] fit in 292.85s\n",
      "→ Evaluate MLP-XSS on X_test=(2738, 50000)\n",
      "{'Model': 'MLP-XSS', 'Pred_s': 0.010081999935209751, 'Precision': 0.999288256227758, 'Recall': 0.951864406779661, 'F1': 0.975, 'FPR': 0.000791765637371338, 'FNR': 0.04813559322033898}\n",
      "--------------------------------------------------\n",
      "MODEL: XGBoost on XSS\n",
      "[XGBoost] fit in 29.42s\n",
      "→ Evaluate XGBoost-XSS on X_test=(2738, 50000)\n",
      "{'Model': 'XGBoost-XSS', 'Pred_s': 0.023783300071954727, 'Precision': 0.6463628396143734, 'Recall': 1.0, 'F1': 0.7852009582113388, 'FPR': 0.6389548693586699, 'FNR': 0.0}\n",
      "--------------------------------------------------\n",
      "MODEL: CatBoost on XSS\n",
      "[CatBoost] fit in 126.37s\n",
      "→ Evaluate CatBoost-XSS on X_test=(2738, 50000)\n",
      "{'Model': 'CatBoost-XSS', 'Pred_s': 0.07700970023870468, 'Precision': 1.0, 'Recall': 0.9464406779661017, 'F1': 0.9724834552420759, 'FPR': 0.0, 'FNR': 0.05355932203389831}\n",
      "--------------------------------------------------\n",
      "MODEL: TabPFN on XSS\n",
      "[TabPFN] train subset: 4000 rows\n",
      "[TabPFN] SVD train=(4000, 150), test=(2738, 150) in 2.61s\n",
      "[TabPFN] fit in 11.25s (device=cuda)\n",
      "→ Evaluate TabPFN-XSS on X_test=(2738, 150)\n",
      "   [predict] batch   1/22 (128 rows) in 40.85s\n",
      "   [predict] batch   2/22 (128 rows) in 41.05s\n",
      "   [predict] batch   3/22 (128 rows) in 40.88s\n",
      "   [predict] batch   4/22 (128 rows) in 40.93s\n",
      "   [predict] batch   5/22 (128 rows) in 40.92s\n",
      "   [predict] batch   6/22 (128 rows) in 40.89s\n",
      "   [predict] batch   7/22 (128 rows) in 40.45s\n",
      "   [predict] batch   8/22 (128 rows) in 38.69s\n",
      "   [predict] batch   9/22 (128 rows) in 38.62s\n",
      "   [predict] batch  10/22 (128 rows) in 38.49s\n",
      "   [predict] batch  11/22 (128 rows) in 38.49s\n",
      "   [predict] batch  12/22 (128 rows) in 38.39s\n",
      "   [predict] batch  13/22 (128 rows) in 38.55s\n",
      "   [predict] batch  14/22 (128 rows) in 38.39s\n",
      "   [predict] batch  15/22 (128 rows) in 38.50s\n",
      "   [predict] batch  16/22 (128 rows) in 38.45s\n",
      "   [predict] batch  17/22 (128 rows) in 38.60s\n",
      "   [predict] batch  18/22 (128 rows) in 38.43s\n",
      "   [predict] batch  19/22 (128 rows) in 38.63s\n",
      "   [predict] batch  20/22 (128 rows) in 38.49s\n",
      "   [predict] batch  21/22 (128 rows) in 38.56s\n",
      "   [predict] batch  22/22 (50 rows) in 84.48s\n",
      "{'Model': 'TabPFN-XSS', 'Pred_s': 909.7467050999403, 'Precision': 1.0, 'Recall': 0.9423728813559322, 'F1': 0.9703315881326352, 'FPR': 0.0, 'FNR': 0.0576271186440678}\n"
     ]
    }
   ],
   "source": [
    "# ===== vorausgesetzt: deine fertigen Splits existieren =====\n",
    "# X_train_sql, X_test_sql, y_train_sql, y_test_sql\n",
    "# X_train_xss, X_test_xss, y_train_xss, y_test_xss\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from catboost import Pool\n",
    "import numpy as np, time, torch\n",
    "\n",
    "\n",
    "splits = {\n",
    "    \"SQL\": (X_train_sql, X_test_sql, y_train_sql, y_test_sql),\n",
    "    \"XSS\": (X_train_xss, X_test_xss, y_train_xss, y_test_xss),\n",
    "}\n",
    "\n",
    "for ds_name, (X_train_txt, X_test_txt, y_train, y_test) in splits.items():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"DATASET: {ds_name} | train={len(y_train)} test={len(y_test)} (pos_rate_train={np.mean(y_train):.3f})\")\n",
    "\n",
    "    #TF-IDF nur auf Train fitten (kein Leakage)\n",
    "    vec = TfidfVectorizer(**VEC_ARGS)\n",
    "    t0 = time.perf_counter()\n",
    "    X_train_vec = vec.fit_transform(X_train_txt)\n",
    "    X_test_vec  = vec.transform(X_test_txt)\n",
    "    print(f\"[VEC] TF-IDF train={X_train_vec.shape}, test={X_test_vec.shape} in {time.perf_counter()-t0:.2f}s\")\n",
    "\n",
    "    #Modelle trainieren/evaluieren (jeweils frischen Klon verwenden)\n",
    "    for name, base_model in models.items():\n",
    "        model = clone(base_model)\n",
    "        print(\"-\"*50 + f\"\\nMODEL: {name} on {ds_name}\")\n",
    "\n",
    "        if name == \"CatBoost\":\n",
    "            # CatBoost: Pool nutzen (sparse ok)\n",
    "            pool_train = Pool(X_train_vec, y_train)\n",
    "            t0 = time.perf_counter()\n",
    "            model.fit(pool_train)\n",
    "            print(f\"[{name}] fit in {time.perf_counter()-t0:.2f}s\")\n",
    "\n",
    "            # Test direkt auf X_test_vec (CatBoost kann CSR); alternativ: Pool(X_test_vec, y_test)\n",
    "            res = evaluate_model(model, X_test_vec, y_test, f\"{name}-{ds_name}\")\n",
    "            results.append(res)\n",
    "            print(res)\n",
    "\n",
    "        elif name == \"TabPFN\":\n",
    "            # TabPFN: ggf. stratifiziertes Subset (Limits), danach SVD -> dichte float32\n",
    "            X_tab, y_tab = X_train_vec, y_train\n",
    "            if X_train_vec.shape[0] > TABPFN_MAX_SAMPLES:\n",
    "                sss = StratifiedShuffleSplit(n_splits=1, train_size=TABPFN_MAX_SAMPLES, random_state=RANDOM_STATE)\n",
    "                idx, _ = next(sss.split(np.zeros(len(y_train)), y_train))\n",
    "                X_tab = X_train_vec[idx]\n",
    "                y_tab = np.asarray(y_train)[idx]\n",
    "            print(f\"[{name}] train subset: {X_tab.shape[0]} rows\")\n",
    "\n",
    "            t0 = time.perf_counter()\n",
    "            svd = TruncatedSVD(n_components=TABPFN_N_COMPONENTS, random_state=RANDOM_STATE)\n",
    "            X_train_svd = svd.fit_transform(X_tab).astype(\"float32\", copy=False)\n",
    "            X_test_svd  = svd.transform(X_test_vec).astype(\"float32\", copy=False)\n",
    "            print(f\"[{name}] SVD train={X_train_svd.shape}, test={X_test_svd.shape} in {time.perf_counter()-t0:.2f}s\")\n",
    "\n",
    "            # Gerät einstellen & fitten\n",
    "            if hasattr(model, \"set_params\"):\n",
    "                model.set_params(device=(\"cuda\" if USE_CUDA else \"cpu\"), ignore_pretraining_limits=True)\n",
    "            t0 = time.perf_counter()\n",
    "            model.fit(X_train_svd, y_tab)\n",
    "            if USE_CUDA:\n",
    "                torch.cuda.synchronize()\n",
    "            print(f\"[{name}] fit in {time.perf_counter()-t0:.2f}s (device={'cuda' if USE_CUDA else 'cpu'})\")\n",
    "\n",
    "            # Evaluieren (wenn deine evaluate_model batched predict unterstützt, ansonsten normal)\n",
    "            try:\n",
    "                res = evaluate_model(model, X_test_svd, y_test, f\"{name}-{ds_name}\",\n",
    "                                     use_batches=True, batch_size=TABPFN_BATCH)\n",
    "            except TypeError:\n",
    "                res = evaluate_model(model, X_test_svd, y_test, f\"{name}-{ds_name}\")\n",
    "            results.append(res)\n",
    "            print(res)\n",
    "\n",
    "        else:\n",
    "            # RF / MLP / XGBoost: direkt auf sparse TF-IDF\n",
    "            t0 = time.perf_counter()\n",
    "            model.fit(X_train_vec, y_train)\n",
    "            print(f\"[{name}] fit in {time.perf_counter()-t0:.2f}s\")\n",
    "            res = evaluate_model(model, X_test_vec, y_test, f\"{name}-{ds_name}\")\n",
    "            results.append(res)\n",
    "            print(res)\n",
    "\n",
    "# Danach kannst du z. B.:\n",
    "# df_results = pd.DataFrame(results)\n",
    "# display(df_results)\n",
    "# df_results.to_excel(\"reports/results_from_presplit.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6574d3c4",
   "metadata": {},
   "source": [
    "# Ergebnisse für Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd8e411",
   "metadata": {},
   "source": [
    "## Ergebnistabelle ausgeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "173bb883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Pred_s</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>FPR</th>\n",
       "      <th>FNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoost-SQL</td>\n",
       "      <td>0.134634</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.787825</td>\n",
       "      <td>0.881322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatBoost-XSS</td>\n",
       "      <td>0.077010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.946441</td>\n",
       "      <td>0.972483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP-SQL</td>\n",
       "      <td>0.021298</td>\n",
       "      <td>0.991189</td>\n",
       "      <td>0.794001</td>\n",
       "      <td>0.881705</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.205999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP-XSS</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>0.999288</td>\n",
       "      <td>0.951864</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.048136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForest-SQL</td>\n",
       "      <td>0.862710</td>\n",
       "      <td>0.991717</td>\n",
       "      <td>0.792236</td>\n",
       "      <td>0.880824</td>\n",
       "      <td>0.003844</td>\n",
       "      <td>0.207764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForest-XSS</td>\n",
       "      <td>0.090351</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.945085</td>\n",
       "      <td>0.971767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TabPFN-SQL</td>\n",
       "      <td>5056.038700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.783855</td>\n",
       "      <td>0.878833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TabPFN-XSS</td>\n",
       "      <td>909.746705</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.942373</td>\n",
       "      <td>0.970332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBoost-SQL</td>\n",
       "      <td>0.041133</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.423908</td>\n",
       "      <td>0.595415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.576092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBoost-XSS</td>\n",
       "      <td>0.023783</td>\n",
       "      <td>0.646363</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.785201</td>\n",
       "      <td>0.638955</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model       Pred_s  Precision    Recall        F1       FPR  \\\n",
       "0      CatBoost-SQL     0.134634   1.000000  0.787825  0.881322  0.000000   \n",
       "1      CatBoost-XSS     0.077010   1.000000  0.946441  0.972483  0.000000   \n",
       "2           MLP-SQL     0.021298   0.991189  0.794001  0.881705  0.004100   \n",
       "3           MLP-XSS     0.010082   0.999288  0.951864  0.975000  0.000792   \n",
       "4  RandomForest-SQL     0.862710   0.991717  0.792236  0.880824  0.003844   \n",
       "5  RandomForest-XSS     0.090351   1.000000  0.945085  0.971767  0.000000   \n",
       "6        TabPFN-SQL  5056.038700   1.000000  0.783855  0.878833  0.000000   \n",
       "7        TabPFN-XSS   909.746705   1.000000  0.942373  0.970332  0.000000   \n",
       "8       XGBoost-SQL     0.041133   1.000000  0.423908  0.595415  0.000000   \n",
       "9       XGBoost-XSS     0.023783   0.646363  1.000000  0.785201  0.638955   \n",
       "\n",
       "        FNR  \n",
       "0  0.212175  \n",
       "1  0.053559  \n",
       "2  0.205999  \n",
       "3  0.048136  \n",
       "4  0.207764  \n",
       "5  0.054915  \n",
       "6  0.216145  \n",
       "7  0.057627  \n",
       "8  0.576092  \n",
       "9  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ergebnisse gespeichert: results_all.csv / results_all.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Ergebnis-Tabelle ausgeben\n",
    "df_results = pd.DataFrame(results)\n",
    "display(df_results.sort_values([\"Model\"]).reset_index(drop=True))\n",
    "df_results.to_csv(\"results_all.csv\", index=False)\n",
    "with pd.ExcelWriter(\"results_all.xlsx\") as w:\n",
    "    df_results.to_excel(w, sheet_name=\"All\", index=False)\n",
    "print(\" Ergebnisse gespeichert: results_all.csv / results_all.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f721a",
   "metadata": {},
   "source": [
    "## Ergebnisse speichern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4caea88",
   "metadata": {},
   "source": [
    "## Ergebnistabelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0360229e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Gesamt Ergebnisse: \n",
      "               Model       Pred_s  Precision    Recall        F1       FPR  \\\n",
      "3      CatBoost-SQL     0.134634   1.000000  0.787825  0.881322  0.000000   \n",
      "8      CatBoost-XSS     0.077010   1.000000  0.946441  0.972483  0.000000   \n",
      "1           MLP-SQL     0.021298   0.991189  0.794001  0.881705  0.004100   \n",
      "6           MLP-XSS     0.010082   0.999288  0.951864  0.975000  0.000792   \n",
      "0  RandomForest-SQL     0.862710   0.991717  0.792236  0.880824  0.003844   \n",
      "5  RandomForest-XSS     0.090351   1.000000  0.945085  0.971767  0.000000   \n",
      "4        TabPFN-SQL  5056.038700   1.000000  0.783855  0.878833  0.000000   \n",
      "9        TabPFN-XSS   909.746705   1.000000  0.942373  0.970332  0.000000   \n",
      "2       XGBoost-SQL     0.041133   1.000000  0.423908  0.595415  0.000000   \n",
      "7       XGBoost-XSS     0.023783   0.646363  1.000000  0.785201  0.638955   \n",
      "\n",
      "        FNR  \n",
      "3  0.212175  \n",
      "8  0.053559  \n",
      "1  0.205999  \n",
      "6  0.048136  \n",
      "0  0.207764  \n",
      "5  0.054915  \n",
      "4  0.216145  \n",
      "9  0.057627  \n",
      "2  0.576092  \n",
      "7  0.000000  \n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.sort_values([\"Model\"], inplace=True)\n",
    "print(\"\\n Gesamt Ergebnisse: \\n\", results_df)\n",
    "\n",
    "results_df.to_csv(\"results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824c2ae6",
   "metadata": {},
   "source": [
    "# Hyperparameter-Tuning & Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6085f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import (\n",
    "    HalvingRandomSearchCV, train_test_split, StratifiedKFold, StratifiedShuffleSplit, RandomizedSearchCV)\n",
    "from joblib import Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00ff0c4",
   "metadata": {},
   "source": [
    "## Feature und Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d23aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-Builder (schnell & schlank)\n",
    "\n",
    "def make_tfidf(max_features=20_000):\n",
    "    return TfidfVectorizer(\n",
    "        analyzer=\"char\", ngram_range=(3,5),\n",
    "        min_df=3, max_features=max_features,\n",
    "        sublinear_tf=True, lowercase=False,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "# Cache, damit TF-IDF/SVD nicht in jedem Kandidaten neu gelernt werden müssen\n",
    "memory = Memory(location=\"skcache\", verbose=0)\n",
    "\n",
    "\n",
    "#schlanke Suchräume\n",
    "def pos_weight(y):\n",
    "    pos = max(1, int(np.sum(y))); neg = max(1, int(len(y)-pos))\n",
    "    return neg / pos\n",
    "\n",
    "def grids(y):\n",
    "    return {\n",
    "        \"RF\": {\n",
    "            \"svd__n_components\": [120, 150, 200],\n",
    "            \"clf__n_estimators\": [300, 600],\n",
    "            \"clf__max_depth\": [None, 20, 40],\n",
    "            \"clf__min_samples_split\": [2, 5, 10],\n",
    "            \"clf__max_features\": [\"sqrt\", None],\n",
    "        },\n",
    "        \"MLP\": {\n",
    "            \"svd__n_components\": [120, 150, 200],\n",
    "            \"clf__hidden_layer_sizes\": [(256,128), (512,256)],\n",
    "            \"clf__alpha\": np.logspace(-5, -3, 3),\n",
    "            \"clf__learning_rate_init\": [1e-4, 5e-4, 1e-3],\n",
    "            \"clf__batch_size\": [64, 128],\n",
    "        },\n",
    "        \"XGB\": {\n",
    "            \"tfidf__max_features\": [15_000, 20_000],\n",
    "            \"clf__n_estimators\": [300, 500],\n",
    "            \"clf__max_depth\": [4, 6, 8],\n",
    "            \"clf__learning_rate\": [0.05, 0.1],\n",
    "            \"clf__subsample\": [0.7, 1.0],\n",
    "            \"clf__colsample_bytree\": [0.6, 1.0],\n",
    "            \"clf__reg_lambda\": [0, 5],\n",
    "            \"clf__gamma\": [0, 1],\n",
    "            \"clf__scale_pos_weight\": [pos_weight(y)],\n",
    "        },\n",
    "        \"CAT\": {\n",
    "            \"tfidf__max_features\": [15_000, 20_000],\n",
    "            \"clf__iterations\": [400, 800],\n",
    "            \"clf__depth\": [6, 8],\n",
    "            \"clf__learning_rate\": [0.05, 0.1],\n",
    "            \"clf__l2_leaf_reg\": [3, 5],\n",
    "        }\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561609f2",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5f4bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipelines\n",
    "def pipe_rf_svd(max_features=20_000, n_comp=150):\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", make_tfidf(max_features)),\n",
    "        (\"svd\", TruncatedSVD(n_components=n_comp, random_state=RANDOM_STATE)),\n",
    "        (\"clf\", RandomForestClassifier(n_jobs=-1, random_state=RANDOM_STATE))\n",
    "    ], memory=memory)\n",
    "\n",
    "def pipe_mlp_svd(max_features=20_000, n_comp=150):\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", make_tfidf(max_features)),\n",
    "        (\"svd\", TruncatedSVD(n_components=n_comp, random_state=RANDOM_STATE)),\n",
    "        (\"scaler\", StandardScaler(with_mean=True)),\n",
    "        (\"clf\", MLPClassifier(\n",
    "            early_stopping=True, n_iter_no_change=8, max_iter=200,\n",
    "            random_state=RANDOM_STATE\n",
    "        ))\n",
    "    ], memory=memory)\n",
    "\n",
    "def pipe_xgb_sparse(max_features=20_000):\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", make_tfidf(max_features)),\n",
    "        (\"clf\", XGBClassifier(\n",
    "            tree_method=\"hist\",\n",
    "            device=(\"cuda\" if USE_CUDA else \"cpu\"),\n",
    "            n_estimators=400,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=1  # Parallelisierung über CV, nicht im Estimator\n",
    "        ))\n",
    "    ], memory=memory)\n",
    "\n",
    "def pipe_cat_sparse(max_features=20_000):\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", make_tfidf(max_features)),\n",
    "        (\"clf\", CatBoostClassifier(\n",
    "            loss_function=\"Logloss\",\n",
    "            verbose=False,\n",
    "            random_seed=RANDOM_STATE\n",
    "        ))\n",
    "    ], memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19503d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Gemeinsames CV-Objekt für faire Vergleiche\n",
    "cv_shared = StratifiedKFold(n_splits=2, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Hilfsfunktion für Hyperparameter-Suche\n",
    "def make_search(estimator, param_distributions, y_train, use_halving=True, cv=None):\n",
    "    if cv is None:\n",
    "        cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scorer = make_scorer(f1_score)\n",
    "    if use_halving:\n",
    "        n = len(y_train)\n",
    "        min_res = max(200, int(0.15 * n))\n",
    "        return HalvingRandomSearchCV(\n",
    "            estimator=estimator,\n",
    "            param_distributions=param_distributions,\n",
    "            resource=\"n_samples\",\n",
    "            min_resources=min_res,\n",
    "            max_resources=\"auto\",\n",
    "            factor=3,\n",
    "            scoring=scorer,\n",
    "            cv=cv,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=2,\n",
    "            verbose=2\n",
    "        )\n",
    "    else:\n",
    "        return RandomizedSearchCV(\n",
    "            estimator=estimator,\n",
    "            param_distributions=param_distributions,\n",
    "            n_iter=10,\n",
    "            scoring=scorer,\n",
    "            cv=cv,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=2,\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "# Training + Suche + Refit + Threshold\n",
    "def tune_one(name, X_train_txt, y_train, cv):\n",
    "    if name == \"RF\":   pipe = pipe_rf_svd()\n",
    "    elif name == \"MLP\": pipe = pipe_mlp_svd()\n",
    "    elif name == \"XGB\": pipe = pipe_xgb_sparse()\n",
    "    elif name == \"CAT\": pipe = pipe_cat_sparse()\n",
    "    else: raise ValueError(name)\n",
    "\n",
    "    # Teilmenge für HPO\n",
    "    X_hpo, y_hpo = get_hpo_subset(X_train_txt, y_train, n=8000)\n",
    "    space = grids(y_hpo)[name]\n",
    "    search = make_search(pipe, space, y_hpo, use_halving=True, cv=cv)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    search.fit(X_hpo, y_hpo)\n",
    "    dt = time.perf_counter() - t0\n",
    "    print(f\"[{name}] best F1 (CV): {search.best_score_:.4f} in {dt/60:.1f} min\")\n",
    "    print(f\"[{name}] best params:\", search.best_params_)\n",
    "\n",
    "    best = search.best_estimator_.set_params(**search.best_params_)\n",
    "    best.fit(X_train_txt, y_train)\n",
    "    return best, search.best_params_, search.best_score_, dt\n",
    "\n",
    "# Threshold-Optimierung + Testauswertung\n",
    "def eval_on_test(name, est, X_train_txt, y_train, X_test_txt, y_test, tune_threshold=True):\n",
    "    thr = 0.5\n",
    "    if tune_threshold and hasattr(est, \"predict_proba\"):\n",
    "        X_tr2, X_val, y_tr2, y_val = train_test_split(X_train_txt, y_train, test_size=0.15, stratify=y_train, random_state=RANDOM_STATE)\n",
    "        est_for_thr = est\n",
    "        est_for_thr.fit(X_tr2, y_tr2)\n",
    "        p = est_for_thr.predict_proba(X_val)[:, 1]\n",
    "        grid = np.linspace(0.1, 0.9, 81)\n",
    "        thr = float(grid[int(np.argmax([f1_score(y_val, (p >= t).astype(int)) for t in grid]))])\n",
    "\n",
    "    y_pred = (est.predict_proba(X_test_txt)[:,1] >= thr).astype(int) if hasattr(est, \"predict_proba\") else est.predict(X_test_txt)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\", zero_division=0)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    fpr = float(fp / (fp + tn)) if (fp + tn) else 0.0\n",
    "    fnr = float(fn / (fn + tp)) if (fn + tp) else 0.0\n",
    "    return dict(Model=name, Thr=thr, Precision=p, Recall=r, F1=f1, FPR=fpr, FNR=fnr)\n",
    "\n",
    "# Ausführung mit übergebenem Split\n",
    "def run_fast_from_split(X_train_txt, X_test_txt, y_train, y_test, ds_name, cv):\n",
    "    results = []\n",
    "    for short, label in [(\"RF\", \"RandomForest\"), (\"MLP\", \"MLP\"), (\"XGB\", \"XGBoost\"), (\"CAT\", \"CatBoost\")]:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"TUNING: {label} ({ds_name})\")\n",
    "        best_est, params, best_cv, dt = tune_one(short, X_train_txt, y_train, cv=cv)\n",
    "        res = eval_on_test(f\"{label}-{ds_name}\", best_est, X_train_txt, y_train, X_test_txt, y_test)\n",
    "        print(\"TEST:\", res)\n",
    "        results.append(res)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Anwendung auf beide Datensätze\n",
    "sql_res = run_fast_from_split(X_train_sql, X_test_sql, y_train_sql, y_test_sql, ds_name=\"SQL\", cv=cv_shared)\n",
    "xss_res = run_fast_from_split(X_train_xss, X_test_xss, y_train_xss, y_test_xss, ds_name=\"XSS\", cv=cv_shared)\n",
    "\n",
    "display(sql_res)\n",
    "display(xss_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- AUSFÜHREN & Excel speichern ----------\n",
    "from datetime import datetime\n",
    "\n",
    "all_test, all_cv = [], []\n",
    "for ds_name, df in [(\"SQL\", sql_df), (\"XSS\", xss_df)]:\n",
    "    df_test, df_cv = run_all_for_dataset(df, ds_name)\n",
    "    all_test.append(df_test); all_cv.append(df_cv)\n",
    "\n",
    "df_test_all = pd.concat(all_test, ignore_index=True)\n",
    "df_cv_all   = pd.concat(all_cv, ignore_index=True)\n",
    "\n",
    "display(df_test_all); display(df_cv_all)\n",
    "\n",
    "# Excel-Export\n",
    "outdir = Path(\"reports\"); outdir.mkdir(parents=True, exist_ok=True)\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "xlsx_path = outdir / f\"results_hpo_{ts}.xlsx\"\n",
    "with pd.ExcelWriter(xlsx_path, engine=\"xlsxwriter\") as w:\n",
    "    df_test_all.to_excel(w, sheet_name=\"Test\", index=False)\n",
    "    df_cv_all.to_excel(w, sheet_name=\"CV\",   index=False)\n",
    "print(\" Saved:\", xlsx_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
